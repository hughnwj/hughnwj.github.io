<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="1. 基础面1. 目前主流的开源模型体系有哪些？ encoder-decoder：bart causal-decoder：llama prefix-decoder：chatglm  2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？ encoder-decoder：编码器双向 + 解码器单向，全局信息编码能力强但参数量大，训练成本">
<meta property="og:type" content="article">
<meta property="og:title" content="0.大模型面经">
<meta property="og:url" content="http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/index.html">
<meta property="og:site_name" content="cozy">
<meta property="og:description" content="1. 基础面1. 目前主流的开源模型体系有哪些？ encoder-decoder：bart causal-decoder：llama prefix-decoder：chatglm  2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？ encoder-decoder：编码器双向 + 解码器单向，全局信息编码能力强但参数量大，训练成本">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250427225201317.png">
<meta property="og:image" content="http://example.com/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250503204636473.png">
<meta property="og:image" content="http://example.com/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250503204728874.png">
<meta property="article:published_time" content="2025-04-27T14:07:15.425Z">
<meta property="article:modified_time" content="2025-05-03T13:01:10.795Z">
<meta property="article:author" content="cozy">
<meta property="article:tag" content="工作">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250427225201317.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>0.大模型面经</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="目录"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="顶部" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/hughnwj">项目</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇" href="/2025/05/04/1.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E8%AF%95%E9%A2%98/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇" href="/2025/04/26/24.DeepSeek%E4%B8%93%E9%A2%98%E4%B9%8B%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%99%BA%E8%83%BD%E4%BD%93%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&text=0.大模型面经"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&is_video=false&description=0.大模型面经"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=0.大模型面经&body=Check out this article: http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&name=0.大模型面经&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&t=0.大模型面经"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%9F%BA%E7%A1%80%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">1. 基础面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E4%BD%93%E7%B3%BB%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">1. 目前主流的开源模型体系有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-prefix-Decoder-%E5%92%8C-causal-Decoder-%E5%92%8C-Encoder-Decoder-%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E7%9A%84-%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">3.大模型LLM的 训练目标 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B%E6%98%AF%E5%95%A5%E5%8E%9F%E5%9B%A0%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4. 涌现能力是啥原因？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%B8%BA%E4%BD%95%E7%8E%B0%E5%9C%A8%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%A4%A7%E9%83%A8%E5%88%86%E6%98%AFDecoder-only%E7%BB%93%E6%9E%84%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5. 为何现在的大模型大部分是Decoder only结构？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E8%BF%9B%E9%98%B6%E9%9D%A2"><span class="toc-number">2.</span> <span class="toc-text">2. 进阶面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">1. 什么是 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">2. 为什么会出现 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.3.</span> <span class="toc-text">3. 如何缓解 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E7%94%A8Bert%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E7%94%A8LLaMA%E3%80%81ChatGLM%E7%B1%BB%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%92%8B%E9%80%89%EF%BC%9F"><span class="toc-number">2.4.</span> <span class="toc-text">4. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%A4%84%E7%90%86%E6%9B%B4%E9%95%BF%E7%9A%84%E6%96%87%E6%9C%AC"><span class="toc-number">2.5.</span> <span class="toc-text">5. 如何让大模型处理更长的文本</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E8%AF%84%E6%B5%8B%E9%9D%A2"><span class="toc-number">3.</span> <span class="toc-text">3. 评测面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%80%8E%E4%B9%88%E8%AF%84%E6%B5%8B"><span class="toc-number">3.1.</span> <span class="toc-text">1. 大模型怎么评测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2. 大模型评估方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%B7%A5%E5%85%B7%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">3. 大模型评估工具有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-agent%E9%9D%A2"><span class="toc-number">4.</span> <span class="toc-text">4. agent面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-agent-%E6%9C%89%E5%93%AA%E4%BA%9B%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90"><span class="toc-number">4.1.</span> <span class="toc-text">1. agent 有哪些部分组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BB%80%E4%B9%88%E6%98%AFPlanning"><span class="toc-number">4.2.</span> <span class="toc-text">2. 什么是Planning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%B9%BB%E8%A7%89%E9%9D%A2"><span class="toc-number">5.</span> <span class="toc-text">5. 幻觉面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1. 什么是大模型幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88LLM%E4%BC%9A%E4%BA%A7%E7%94%9F%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">2. 为什么LLM会产生幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3LLM%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3. 如何缓解LLM幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-LLMs%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E6%9C%80%E5%AE%B9%E6%98%93%E4%BA%A7%E7%94%9F%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4. LLMs什么时候最容易产生幻觉？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-RLHF%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.</span> <span class="toc-text">6. 强化学习-RLHF人类反馈强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-LLM%E7%9A%84%E7%BB%8F%E5%85%B8%E9%A2%84%E8%AE%AD%E7%BB%83Pipeline%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">1. 介绍一下 LLM的经典预训练Pipeline？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%85%B7%E4%BD%93%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2. 具体介绍一下预训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%85%B7%E4%BD%93%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="toc-number">6.3.</span> <span class="toc-text">3. 具体介绍一下有监督微调？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E5%AF%B9%E9%BD%90%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4. 简单介绍一下对齐？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-RLHF-%E6%B5%81%E7%A8%8B%EF%BC%9F"><span class="toc-number">6.5.</span> <span class="toc-text">5. 简单介绍一下 RLHF 流程？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A6%82%E4%BD%95%E5%9C%A8%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AARM%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">6.6.</span> <span class="toc-text">6. 如何在有监督微调模型基础上创建一个RM模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BRLHF%E4%B8%AD%E7%9A%84PPO%E4%B8%BB%E8%A6%81%E5%88%86%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="toc-number">6.7.</span> <span class="toc-text">7. 大语言模型RLHF中的PPO主要分哪些步骤？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%B8%BE%E4%BE%8B%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84RLHF%EF%BC%9F"><span class="toc-number">6.8.</span> <span class="toc-text">8. 举例描述一下大语言模型的RLHF？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-PPO%E9%87%87%E6%A0%B7%E8%BF%87%E7%A8%8B"><span class="toc-number">6.9.</span> <span class="toc-text">8. PPO采样过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-PPO-%E4%B8%AD-%E9%87%87%E6%A0%B7%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-number">6.10.</span> <span class="toc-text">9.介绍一下 PPO 中 采样策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-instructGPT%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%8C%E8%AE%B2%E8%AE%B2rlhf%E5%92%8Creward%EF%BC%9F"><span class="toc-number">6.11.</span> <span class="toc-text">10. instructGPT的原理，讲讲rlhf和reward？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E8%AE%AD%E7%BB%83%E9%9B%86%E9%9D%A2"><span class="toc-number">7.</span> <span class="toc-text">7. 训练集面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT%EF%BC%88%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%89%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">1. SFT（有监督微调）的数据集格式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-RM%EF%BC%88%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%89%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">2. RM（奖励模型）的数据格式？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E6%8E%A8%E7%90%86%E9%9D%A2"><span class="toc-number">8.</span> <span class="toc-text">8. 推理面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E4%BC%B0%E7%AE%97%E6%A8%A1%E5%9E%8B%E6%89%80%E9%9C%80%E7%9A%84RAM%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">1. 如何估算模型所需的RAM？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-%E5%BE%AE%E8%B0%83%E9%9D%A2"><span class="toc-number">9.</span> <span class="toc-text">9. 微调面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%A2%86%E5%9F%9F%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83%E5%90%8E%EF%BC%8C%E9%80%9A%E7%94%A8%E8%83%BD%E5%8A%9B%E5%BE%80%E5%BE%80%E4%BC%9A%E6%9C%89%E6%89%80%E4%B8%8B%E9%99%8D%EF%BC%8C%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E6%A8%A1%E5%9E%8B%E9%81%97%E5%BF%98%E9%80%9A%E7%94%A8%E8%83%BD%E5%8A%9B%EF%BC%9F"><span class="toc-number">9.1.</span> <span class="toc-text">1. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%87%BA%E7%8E%B0%E8%83%BD%E5%8A%9B%E5%8A%A3%E5%8C%96%EF%BC%8C%E7%81%BE%E9%9A%BE%E6%80%A7%E9%81%97%E5%BF%98%E6%98%AF%E6%80%8E%E4%B9%88%E5%9B%9E%E4%BA%8B%EF%BC%9F"><span class="toc-number">9.2.</span> <span class="toc-text">2.  微调后的模型出现能力劣化，灾难性遗忘是怎么回事？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C-batch-size-%E8%AE%BE%E7%BD%AE%E5%A4%AA%E5%B0%8F-%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">9.3.</span> <span class="toc-text">3. 微调大模型时，如果 batch size 设置太小 会出现什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C-batch-size-%E8%AE%BE%E7%BD%AE%E5%A4%AA%E5%A4%A7-%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">9.4.</span> <span class="toc-text">4.  微调大模型时，如果 batch size 设置太大 会出现什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6-%E4%BC%98%E5%8C%96%E5%99%A8%E5%A6%82%E4%BD%95%EF%BC%9F"><span class="toc-number">9.5.</span> <span class="toc-text">5.  微调大模型时, 优化器如何？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83loss%E7%AA%81%E5%88%BA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">9.6.</span> <span class="toc-text">6. 大模型训练loss突刺是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%87%BA%E7%8E%B0loss%E7%AA%81%E5%88%BA%EF%BC%9F"><span class="toc-number">9.7.</span> <span class="toc-text">7. 为什么大模型训练会出现loss突刺？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83loss%E7%AA%81%E5%88%BA-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">9.8.</span> <span class="toc-text">8. 大模型训练loss突刺 如何解决？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-LoRA%E9%9D%A2"><span class="toc-number">10.</span> <span class="toc-text">10. LoRA面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-LoRA%EF%BC%9F"><span class="toc-number">10.1.</span> <span class="toc-text">1.  什么是 LoRA？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.2.</span> <span class="toc-text">2. LoRA 的思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-LoRA-%E7%9A%84%E7%89%B9%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.3.</span> <span class="toc-text">3. LoRA 的特点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-QLoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">10.4.</span> <span class="toc-text">4. QLoRA 的思路是怎么样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-AdaLoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">10.5.</span> <span class="toc-text">5. AdaLoRA 的思路是怎么样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-LoRA%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95%E4%B8%BA%E5%95%A5%E8%83%BD%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">10.6.</span> <span class="toc-text">6. LoRA微调方法为啥能加速训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%B2%E6%9C%89LoRA%E6%A8%A1%E5%9E%8B%E4%B8%8A%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">10.7.</span> <span class="toc-text">7.  如何在已有LoRA模型上继续训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-LoRA-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.8.</span> <span class="toc-text">8. LoRA 缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Rank-%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%EF%BC%9F"><span class="toc-number">10.9.</span> <span class="toc-text">9. Rank 如何选取？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-alpha%E5%8F%82%E6%95%B0-%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%EF%BC%9F"><span class="toc-number">10.10.</span> <span class="toc-text">10. alpha参数 如何选取？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-LoRA-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83-%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="toc-number">10.11.</span> <span class="toc-text">11. LoRA 高效微调 如何避免过拟合？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Lora%E7%9A%84%E7%9F%A9%E9%98%B5%E6%80%8E%E4%B9%88%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.12.</span> <span class="toc-text">12. Lora的矩阵怎么初始化？为什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">11. 提示学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89%EF%BC%9F"><span class="toc-number">11.1.</span> <span class="toc-text">1. 什么是 提示学习（Prompting）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89%EF%BC%9F"><span class="toc-number">11.2.</span> <span class="toc-text">2. 为什么需要 提示学习（Prompting）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9%EF%BC%9F"><span class="toc-number">11.3.</span> <span class="toc-text">3. 提示学习（Prompting） 有什么优点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%EF%BC%9F"><span class="toc-number">11.4.</span> <span class="toc-text">4. 为什么需要 前缀微调（Prefix-tining）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.5.</span> <span class="toc-text">5. 前缀微调（Prefix-tining）思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E7%9A%84%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.6.</span> <span class="toc-text">6. 前缀微调（Prefix-tining）的优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E7%9A%84%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.7.</span> <span class="toc-text">7.  前缀微调（Prefix-tining）的缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%EF%BC%9F"><span class="toc-number">11.8.</span> <span class="toc-text">8. 为什么需要 指示微调（Prompt-tuning）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.9.</span> <span class="toc-text">9. 指示微调（Prompt-tuning）思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.10.</span> <span class="toc-text">10. 指示微调（Prompt-tuning）优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.11.</span> <span class="toc-text">11. 指示微调（Prompt-tuning）缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%B8%8E-Prefix-tuning-%E5%8C%BA%E5%88%AB-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.12.</span> <span class="toc-text">12.  指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%B8%8E-fine-tuning-%E5%8C%BA%E5%88%AB-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.13.</span> <span class="toc-text">13. 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-P-tuning%EF%BC%9F"><span class="toc-number">11.14.</span> <span class="toc-text">14. 为什么需要 P-tuning？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-P-tuning-%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.15.</span> <span class="toc-text">15. P-tuning 思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-P-tuning-%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.16.</span> <span class="toc-text">16. P-tuning 优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-P-tuning-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.17.</span> <span class="toc-text">17. P-tuning 缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-P-tuning-v2%EF%BC%9F"><span class="toc-number">11.18.</span> <span class="toc-text">18. 为什么需要 P-tuning v2？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-P-tuning-v2-%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.19.</span> <span class="toc-text">19. P-tuning v2 思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-P-tuning-v2-%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.20.</span> <span class="toc-text">20. P-tuning v2 优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-P-tuning-v2-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.21.</span> <span class="toc-text">21. P-tuning v2 缺点是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-langchain%E9%9D%A2"><span class="toc-number">12.</span> <span class="toc-text">12. langchain面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Components-and-Chains"><span class="toc-number">12.1.</span> <span class="toc-text">1. Components and Chains</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Prompt-Templates-and-Values"><span class="toc-number">12.2.</span> <span class="toc-text">2. Prompt Templates and Values</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Example-Selectors"><span class="toc-number">12.3.</span> <span class="toc-text">3. Example Selectors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Output-Parsers"><span class="toc-number">12.4.</span> <span class="toc-text">4. Output Parsers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Indexes-and-Retrievers"><span class="toc-number">12.5.</span> <span class="toc-text">5. Indexes and Retrievers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Chat-Message-History"><span class="toc-number">12.6.</span> <span class="toc-text">6. Chat Message History</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Agents-and-Toolkits"><span class="toc-number">12.7.</span> <span class="toc-text">7. Agents and Toolkits</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-LangChain-%E5%A6%82%E4%BD%95Embedding-vector-store%EF%BC%9F"><span class="toc-number">12.8.</span> <span class="toc-text">8. LangChain 如何Embedding &amp; vector store？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Tokenizer%E9%9D%A2"><span class="toc-number">13.</span> <span class="toc-text">13. Tokenizer面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Tokenizer-%E4%BB%8B%E7%BB%8D"><span class="toc-number">13.1.</span> <span class="toc-text">1. Tokenizer 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Byte-Pair-Encoding-BPE"><span class="toc-number">13.2.</span> <span class="toc-text">2. Byte-Pair Encoding(BPE)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-WordPiece-%E4%B8%8E-BPE-%E5%BC%82%E5%90%8C%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">13.3.</span> <span class="toc-text">3. WordPiece 与 BPE 异同点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-SentencePiece-%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="toc-number">13.4.</span> <span class="toc-text">4. 简单介绍一下 SentencePiece 思路？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%9D%A2"><span class="toc-number">14.</span> <span class="toc-text">14. 分布式训练面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E6%9E%9C%E6%9C%89N%E5%BC%A0%E6%98%BE%E5%AD%98%E8%B6%B3%E5%A4%9F%E5%A4%A7%E7%9A%84%E6%98%BE%E5%8D%A1%EF%BC%8C%E6%80%8E%E4%B9%88%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">14.1.</span> <span class="toc-text">1. 如果有N张显存足够大的显卡，怎么加速训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E6%9E%9C%E6%98%BE%E5%8D%A1%E7%9A%84%E6%98%BE%E5%AD%98%E4%B8%8D%E5%A4%9F%E8%A3%85%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%91%A2%EF%BC%9F"><span class="toc-number">14.2.</span> <span class="toc-text">2. 如果显卡的显存不够装下一个完整的模型呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PP%E6%8E%A8%E7%90%86%E6%97%B6%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E4%B8%B2%E8%A1%8C%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%8C1%E4%B8%AAGPU%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%85%B6%E4%BB%96%E7%A9%BA%E9%97%B2%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F%EF%BC%9F"><span class="toc-number">14.3.</span> <span class="toc-text">3. PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3D-%E5%B9%B6%E8%A1%8C"><span class="toc-number">14.4.</span> <span class="toc-text">4. 3D 并行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%9D%A2"><span class="toc-number">15.</span> <span class="toc-text">15. 推理加速面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BD%93%E5%89%8D%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E6%9C%80%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF%E6%89%8B%E6%AE%B5%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.1.</span> <span class="toc-text">1. 当前优化模型最主要技术手段有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%A1%86%E6%9E%B6%E6%9C%89%E5%93%AA%E4%B8%80%E4%BA%9B%EF%BC%9F%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">15.2.</span> <span class="toc-text">2. 推理加速框架有哪一些？都有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-vLLM-%E7%9A%84-%E5%8A%9F%E8%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.3.</span> <span class="toc-text">3. vLLM 的 功能有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-vLLM-%E7%9A%84-%E4%BC%98%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.4.</span> <span class="toc-text">4. vLLM 的 优点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-vLLM-%E7%9A%84-%E7%BC%BA%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.5.</span> <span class="toc-text">5. vLLM 的 缺点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-Text-generation-inference%EF%BC%9F"><span class="toc-number">15.6.</span> <span class="toc-text">6. 介绍一下 Text generation inference？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Text-generation-inference-%E7%9A%84-%E5%8A%9F%E8%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.7.</span> <span class="toc-text">7. Text generation inference 的 功能有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Text-generation-inference-%E7%9A%84-%E4%BC%98%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.8.</span> <span class="toc-text">8. Text generation inference 的 优点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Text-generation-inference-%E7%9A%84-%E7%BC%BA%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.9.</span> <span class="toc-text">9. Text generation inference 的 缺点有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-%E6%98%BE%E5%AD%98%E9%97%AE%E9%A2%98%E9%9D%A2"><span class="toc-number">16.</span> <span class="toc-text">16. 显存问题面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-nB-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%98%BE%E5%AD%98%EF%BC%9F"><span class="toc-number">16.1.</span> <span class="toc-text">1. nB 模型训练需要多少显存？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E9%9D%A2"><span class="toc-number">17.</span> <span class="toc-text">17. 增量预训练面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">17.1.</span> <span class="toc-text">1. 为什么要增量预训练？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-%E8%92%B8%E9%A6%8F%E9%9D%A2"><span class="toc-number">18.</span> <span class="toc-text">18. 蒸馏面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A0%B7%E6%9C%AC%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">18.1.</span> <span class="toc-text">1. 知识蒸馏和无监督样本训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AF%B9%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9F%A5%E9%81%93%E5%A4%9A%E5%B0%91%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E6%94%B9%E8%BF%9B%E7%94%A8%E5%88%B0%E4%BA%86%EF%BC%9F"><span class="toc-number">18.2.</span> <span class="toc-text">2. 对知识蒸馏知道多少，有哪些改进用到了？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%B0%88%E4%B8%80%E4%B8%8B%E5%AF%B9%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E7%9A%84%E4%BA%86%E8%A7%A3%EF%BC%9F"><span class="toc-number">18.3.</span> <span class="toc-text">3. 谈一下对模型量化的了解？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%8A%A0%E9%80%9F%E7%9A%84%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">18.4.</span> <span class="toc-text">4. 模型压缩和加速的方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%A0%E4%BA%86%E8%A7%A3%E7%9A%84%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">18.5.</span> <span class="toc-text">5. 你了解的知识蒸馏模型有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-RAG%E9%9D%A2"><span class="toc-number">19.</span> <span class="toc-text">19. RAG面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RAG-%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%EF%BC%9F"><span class="toc-number">19.1.</span> <span class="toc-text">1. RAG 思路是怎么样？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%87%E6%A1%A3%E5%88%87%E5%88%86%E7%B2%92%E5%BA%A6%E4%B8%8D%E5%A5%BD%E6%8A%8A%E6%8E%A7%EF%BC%8C%E6%97%A2%E6%8B%85%E5%BF%83%E5%99%AA%E5%A3%B0%E5%A4%AA%E5%A4%9A%E5%8F%88%E6%8B%85%E5%BF%83%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E4%B8%A2%E5%A4%B1"><span class="toc-number">19.2.</span> <span class="toc-text">2. 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%9C%A8%E5%9F%BA%E4%BA%8E%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E8%A1%A8%E7%8E%B0%E4%B8%8D%E4%BD%B3"><span class="toc-number">19.3.</span> <span class="toc-text">3. 在基于垂直领域表现不佳</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-langchain-%E5%86%85%E7%BD%AE%E9%97%AE%E7%AD%94%E5%88%86%E5%8F%A5%E6%95%88%E6%9E%9C%E4%B8%8D%E4%BD%B3%E9%97%AE%E9%A2%98"><span class="toc-number">19.4.</span> <span class="toc-text">4. langchain 内置问答分句效果不佳问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%8F%AC%E5%9B%9E%E4%B8%8E-query-%E7%9B%B8%E5%85%B3%E7%9A%84-Document-%E9%97%AE%E9%A2%98"><span class="toc-number">19.5.</span> <span class="toc-text">5. 如何尽可能召回与 query 相关的 Document 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-embedding-%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%A1%A8%E7%A4%BA-text-chunks-%E6%97%B6%E5%81%8F%E5%B7%AE%E5%A4%AA%E5%A4%A7%E9%97%AE%E9%A2%98"><span class="toc-number">19.6.</span> <span class="toc-text">6. embedding 模型在表示 text chunks 时偏差太大问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">19.7.</span> <span class="toc-text">7. RAG 有哪些评估方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%85%B3%E9%94%AE%E6%8C%87%E6%A0%87%E5%92%8C%E8%83%BD%E5%8A%9B%EF%BC%9F"><span class="toc-number">19.8.</span> <span class="toc-text">8. RAG 有哪些关键指标和能力？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="toc-number">19.9.</span> <span class="toc-text">9. RAG 有哪些评估框架？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-RAG-%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-number">19.10.</span> <span class="toc-text">10. RAG 架构优化有哪些优化策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.11.</span> <span class="toc-text">11.  如何通过混合检索提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E9%87%8D%E6%96%B0%E6%8E%92%E5%90%8D%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.12.</span> <span class="toc-text">12. 如何通过重新排名提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%B7%BB%E5%8A%A0%E5%85%83%E6%95%B0%E6%8D%AE-%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.13.</span> <span class="toc-text">13. 如何通过添加元数据 提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%8F%90%E7%A4%BA%E5%8E%8B%E7%BC%A9%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.14.</span> <span class="toc-text">14. 如何通过提示压缩提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87-%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99%E5%92%8C%E6%89%A9%E5%B1%95-%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.15.</span> <span class="toc-text">15. 如何通过 查询重写和扩展 提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-RAG-%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91"><span class="toc-number">19.16.</span> <span class="toc-text">16. RAG 未来发展方向</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-%E5%A4%9A%E6%A8%A1%E6%80%81%E9%9D%A2"><span class="toc-number">20.</span> <span class="toc-text">20. 多模态面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-blip2%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%BC%98%E5%8A%BF%E5%92%8C%E4%B9%8B%E5%89%8D%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">20.1.</span> <span class="toc-text">1. blip2的架构，优势和之前多模态模型的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84SOTA%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">20.2.</span> <span class="toc-text">2. 多模态中常见的SOTA模型有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bstable-diffusion%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">20.3.</span> <span class="toc-text">3. 介绍一下stable diffusion的原理？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-%E4%BB%A3%E7%A0%81%E9%9D%A2"><span class="toc-number">21.</span> <span class="toc-text">21. 代码面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RMS-Norm-%E7%9B%B8%E6%AF%94%E4%BA%8E-Layer-Norm-%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">21.1.</span> <span class="toc-text">1.  RMS Norm 相比于 Layer Norm 有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Deep-Norm-%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="toc-number">21.2.</span> <span class="toc-text">2.  Deep Norm 思路？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Deep-Norm-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9%EF%BC%9F"><span class="toc-number">21.3.</span> <span class="toc-text">3. Deep Norm 有什么优点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-LN-%E5%9C%A8-LLMs-%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C%E4%BD%8D%E7%BD%AE%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E4%B9%88%EF%BC%9F%E5%A6%82%E6%9C%89%E8%83%BD%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E5%8C%BA%E5%88%AB%E4%B9%88%EF%BC%9F"><span class="toc-number">21.4.</span> <span class="toc-text">4. LN 在 LLMs 中的不同位置有什么区别么？如有能介绍一下区别么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-LLMs-%E5%90%84%E6%A8%A1%E5%9E%8B%E5%88%86%E5%88%AB%E7%94%A8%E4%BA%86%E5%93%AA%E7%A7%8D-Layer-normalization%EF%BC%9F"><span class="toc-number">21.5.</span> <span class="toc-text">5. LLMs 各模型分别用了哪种 Layer normalization？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%90%84-LLMs-%E9%83%BD%E4%BD%BF%E7%94%A8%E5%93%AA%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">21.6.</span> <span class="toc-text">6. 各 LLMs 都使用哪种激活函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%BC%A0%E7%BB%9F-Attention-%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">21.7.</span> <span class="toc-text">7. 传统 Attention 存在哪些问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Attention-%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91%EF%BC%9F"><span class="toc-number">21.8.</span> <span class="toc-text">8. Attention 优化方向？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Attention-%E5%8F%98%E4%BD%93%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">21.9.</span> <span class="toc-text">9. Attention 变体有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Multi-head-Attention-%E5%AD%98%E5%9C%A8%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">21.10.</span> <span class="toc-text">10. Multi-head Attention 存在什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.11.</span> <span class="toc-text">11. 介绍一下 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AF%B9%E6%AF%94%E4%B8%80%E4%B8%8B-Multi-head-Attention-%E5%92%8C-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.12.</span> <span class="toc-text">12. 对比一下 Multi-head Attention 和 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Multi-Query-Attention-%E8%BF%99%E6%A0%B7%E5%81%9A%E7%9A%84%E5%A5%BD%E5%A4%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">21.13.</span> <span class="toc-text">13. Multi-Query Attention 这样做的好处是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A8%A1%E5%9E%8B%E6%98%AF%E4%BD%BF%E7%94%A8-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.14.</span> <span class="toc-text">14. 有哪些模型是使用 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-%E4%BB%80%E4%B9%88%E6%98%AF-Grouped-query-Attention%EF%BC%9F"><span class="toc-number">21.15.</span> <span class="toc-text">15. 什么是 Grouped-query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8-Grouped-query-Attention%EF%BC%9F"><span class="toc-number">21.16.</span> <span class="toc-text">16. 有哪些大模型使用 Grouped-query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-Flash-Attention-%E7%AF%87"><span class="toc-number">21.17.</span> <span class="toc-text">17. Flash Attention 篇</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-%E5%B9%B6%E8%A1%8C-transformer-block"><span class="toc-number">21.18.</span> <span class="toc-text">18. 并行 transformer block</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-KL-%E6%95%A3%E5%BA%A6%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">21.19.</span> <span class="toc-text">19. KL 散度与交叉熵的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E5%90%84-loss-%E5%B7%AE%E5%BC%82%E8%BF%87%E5%A4%A7%E6%80%8E%E6%A0%B7%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">21.20.</span> <span class="toc-text">20. 多任务学习各 loss 差异过大怎样处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8D%E7%94%A8%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89%EF%BC%9F"><span class="toc-number">21.21.</span> <span class="toc-text">21. 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-%E9%99%A4%E4%BA%86-cosin-%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E7%AE%97%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">21.22.</span> <span class="toc-text">22. 除了 cosin 还有哪些算相似度的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-%E4%BA%86%E8%A7%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%98%9B%EF%BC%9F"><span class="toc-number">21.23.</span> <span class="toc-text">23. 了解对比学习嘛？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%98%AF%E5%90%A6%E9%87%8D%E8%A6%81%EF%BC%9F%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%9E%84%E9%80%A0%E6%88%90%E6%9C%AC%E8%BF%87%E9%AB%98%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">21.24.</span> <span class="toc-text">24. 对比学习负样本是否重要？负样本构造成本过高应该怎么解决？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">21.25.</span> <span class="toc-text">25. 分布式训练框架选择？</span></a></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        0.大模型面经
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">cozy</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2025-04-27T14:07:15.425Z" class="dt-published" itemprop="datePublished">2025-04-27</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fa-solid fa-archive"></i>
        <a class="category-link" href="/categories/%E9%9D%A2%E7%BB%8F/">面经</a>
    </div>


      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/%E5%B7%A5%E4%BD%9C/" rel="tag">工作</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="1-基础面"><a href="#1-基础面" class="headerlink" title="1. 基础面"></a>1. 基础面</h1><h2 id="1-目前主流的开源模型体系有哪些？"><a href="#1-目前主流的开源模型体系有哪些？" class="headerlink" title="1. 目前主流的开源模型体系有哪些？"></a>1. 目前主流的开源模型体系有哪些？</h2><ol>
<li>encoder-decoder：bart</li>
<li>causal-decoder：llama</li>
<li>prefix-decoder：chatglm</li>
</ol>
<h2 id="2-prefix-Decoder-和-causal-Decoder-和-Encoder-Decoder-区别是什么？"><a href="#2-prefix-Decoder-和-causal-Decoder-和-Encoder-Decoder-区别是什么？" class="headerlink" title="2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？"></a>2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？</h2><ol>
<li>encoder-decoder：编码器双向 + 解码器单向，全局信息编码能力强但参数量大，训练成本高</li>
<li>causal-decoder：严格单向，生成连贯性强但无法并行编码全局信息</li>
<li>prefix-decoder：前缀双向 + 生成单向，支持部分双向上下文理解但实现复杂度较高</li>
</ol>
<h2 id="3-大模型LLM的-训练目标-是什么？"><a href="#3-大模型LLM的-训练目标-是什么？" class="headerlink" title="3.大模型LLM的 训练目标 是什么？"></a>3.大模型LLM的 训练目标 是什么？</h2><ol>
<li>语言模型</li>
<li>去噪自编码器：恢复被打乱的文本段</li>
</ol>
<h2 id="4-涌现能力是啥原因？"><a href="#4-涌现能力是啥原因？" class="headerlink" title="4. 涌现能力是啥原因？"></a>4. 涌现能力是啥原因？</h2><ol>
<li>评价指标不够平滑；</li>
<li>复杂任务拆分的子任务，子任务效果是平滑增长的，但是最终任务的指标涌现，也就是说宏观上看到了涌现现象。</li>
</ol>
<h2 id="5-为何现在的大模型大部分是Decoder-only结构？"><a href="#5-为何现在的大模型大部分是Decoder-only结构？" class="headerlink" title="5. 为何现在的大模型大部分是Decoder only结构？"></a>5. 为何现在的大模型大部分是Decoder only结构？</h2><ol>
<li>encoder-decoder参数量大，训练成本高</li>
<li>decoder-only结构模型在没有任何微调数据的情况下，zero-shot的表现能力最好</li>
</ol>
<h1 id="2-进阶面"><a href="#2-进阶面" class="headerlink" title="2. 进阶面"></a>2. 进阶面</h1><h2 id="1-什么是-LLMs-复读机问题？"><a href="#1-什么是-LLMs-复读机问题？" class="headerlink" title="1. 什么是 LLMs 复读机问题？"></a>1. 什么是 LLMs 复读机问题？</h2><ol>
<li>对同一个字或者同一个词或者同一句话重复</li>
<li>相同的提示词生成相同相近内容</li>
<li>不同的提示词生成类似内容</li>
</ol>
<h2 id="2-为什么会出现-LLMs-复读机问题？"><a href="#2-为什么会出现-LLMs-复读机问题？" class="headerlink" title="2. 为什么会出现 LLMs 复读机问题？"></a>2. 为什么会出现 LLMs 复读机问题？</h2><ol>
<li><p>数据集：训练数据存在大量重复文本，不够多样化</p>
</li>
<li><p>训练方法：自监督学习</p>
</li>
<li><p>模型架构：注意力机制</p>
</li>
<li><p>参数设置：生成策略</p>
</li>
</ol>
<h2 id="3-如何缓解-LLMs-复读机问题？"><a href="#3-如何缓解-LLMs-复读机问题？" class="headerlink" title="3. 如何缓解 LLMs 复读机问题？"></a>3. 如何缓解 LLMs 复读机问题？</h2><ol>
<li>训练中通过添加损失函数对重复词抑制来减少重复输出</li>
<li>推理中对softmax加入重复惩罚因子</li>
<li>训练添加对比loss，推理进行对比搜索</li>
<li>生成文本时引入噪声</li>
<li>Beam Search：在每一个时间步，不再只保留当前分数最高的1个输出，而是保留num_beams个</li>
</ol>
<p><img src="/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250427225201317.png" alt="image-20250427225201317">：</p>
<ol start="6">
<li>TopK采样：选择最大的K个token，对K个token进行采样</li>
<li>TopP采样：TopP采样会不断选择logit中最大概率的token，放入一个list中，直到list中计算的总概率大于设置的TopP值，后对list中的token概率进行重新计算，最终根据计算出来的 概率值对list中的token进行采样</li>
<li>Temperature：温度为 0 将始终产生相同的输出。执行具有“正确”答案的任务，对于总结类，翻译类等具有明确答案的任务，较低的温度（小于1）更合适。如果模型开始自我重复，则表明温度设置过低。高温意味着更多的随机性，这可以帮助模型给出更有创意的输出。如果模型开始偏离主题或给出无意义的输出，则表明温度过高</li>
<li>通过限制设置的ngram不能出现重复，如果重复，就选概率次大的一个，来强制模型不生成重复的token</li>
<li>对生成的文本用文本相似度计算方法或规则来检测和去除重复的文本。</li>
</ol>
<h2 id="4-什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？"><a href="#4-什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？" class="headerlink" title="4. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？"></a>4. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？</h2><ol>
<li>bert参数小，部署快，处理快。LLaMA、ChatGLM参数大，部署慢，预测慢。</li>
<li>NLU：用bert，实体识别、信息抽取、文本分类</li>
<li>NLG：用LLaMA、ChatGLM类大模型，文本生成</li>
</ol>
<h2 id="5-如何让大模型处理更长的文本"><a href="#5-如何让大模型处理更长的文本" class="headerlink" title="5. 如何让大模型处理更长的文本"></a>5. 如何让大模型处理更长的文本</h2><ol>
<li>优化注意力机制，降低复杂度</li>
<li>位置编码：对绝对位置编码缩放</li>
<li>对文本分块，跨块注意力机制</li>
<li>rag技术，检索只输入关键内容</li>
<li>对文本进行摘要再输入</li>
</ol>
<h1 id="3-评测面"><a href="#3-评测面" class="headerlink" title="3. 评测面"></a>3. 评测面</h1><h2 id="1-大模型怎么评测"><a href="#1-大模型怎么评测" class="headerlink" title="1. 大模型怎么评测"></a>1. 大模型怎么评测</h2><p>对语言生成，知识推理，代码能力，安全伦理方面进行评测</p>
<h2 id="2-大模型评估方法有哪些？"><a href="#2-大模型评估方法有哪些？" class="headerlink" title="2. 大模型评估方法有哪些？"></a>2. 大模型评估方法有哪些？</h2><ol>
<li><p>静态数据集：GLUP</p>
</li>
<li><p>动态对抗：通过对抗，实时聊天，两两比对，人工进行打分</p>
</li>
</ol>
<h2 id="3-大模型评估工具有哪些？"><a href="#3-大模型评估工具有哪些？" class="headerlink" title="3. 大模型评估工具有哪些？"></a>3. 大模型评估工具有哪些？</h2><p>OpenAI evals</p>
<h1 id="4-agent面"><a href="#4-agent面" class="headerlink" title="4. agent面"></a>4. agent面</h1><h2 id="1-agent-有哪些部分组成"><a href="#1-agent-有哪些部分组成" class="headerlink" title="1. agent 有哪些部分组成"></a>1. agent 有哪些部分组成</h2><p>momory，planning，action，tools</p>
<h2 id="2-什么是Planning"><a href="#2-什么是Planning" class="headerlink" title="2. 什么是Planning"></a>2. 什么是Planning</h2><ol>
<li>拆解子目标和任务分解： <ol>
<li>few-shot</li>
<li>思维链提示词</li>
</ol>
</li>
<li>模型自我反省：ReAct<ol>
<li>Thought: …</li>
<li>Action: …</li>
<li>Observation: …</li>
</ol>
</li>
</ol>
<h1 id="5-幻觉面"><a href="#5-幻觉面" class="headerlink" title="5. 幻觉面"></a>5. 幻觉面</h1><h2 id="1-什么是大模型幻觉？"><a href="#1-什么是大模型幻觉？" class="headerlink" title="1. 什么是大模型幻觉？"></a>1. 什么是大模型幻觉？</h2><p>一本正经的胡说八道</p>
<h2 id="2-为什么LLM会产生幻觉？"><a href="#2-为什么LLM会产生幻觉？" class="headerlink" title="2. 为什么LLM会产生幻觉？"></a>2. 为什么LLM会产生幻觉？</h2><ol>
<li>数据：训练数据问题和答案本身就是不对的，或者数据中高频语句以及数据噪声</li>
<li>参数：topk，topp，温度不合理</li>
<li>模型比起上下文更依赖预训练知识</li>
</ol>
<h2 id="3-如何缓解LLM幻觉？"><a href="#3-如何缓解LLM幻觉？" class="headerlink" title="3. 如何缓解LLM幻觉？"></a>3. 如何缓解LLM幻觉？</h2><ol>
<li><p>数据清洗和预处理：在训练大语言模型之前，对数据进行仔细的清洗和预处理是至关重要的。删除不准确、噪声或有偏差的数据可以减少模型幻觉问题的出现。</p>
</li>
<li><p>多样化训练数据：为了减少模型对特定数据源的依赖和偏好，可以尽量使用多样化的训练数据。包括来自不同领域、不同来源和不同观点的数据，以获得更全面的语言理解。</p>
</li>
<li><p>引入多样性的生成策略：在生成文本时，可以采用多样性的生成策略来减少模型的倾向性和幻觉问题。例如，使用温度参数来调整生成的多样性，或者使用抽样和束搜索等不同的生成方法。</p>
</li>
<li><p>人工审核和后处理：对生成的文本进行人工审核和后处理是一种常用的方法。通过人工的干预和修正，可以纠正模型幻觉问题，并确保生成的内容准确和可靠。</p>
</li>
<li><p>引入外部知识和约束：为了提高生成文本的准确性，可以引入外部知识和约束。例如，结合知识图谱、实体识别或逻辑推理等技术，将先验知识和约束融入到生成过程中。</p>
</li>
</ol>
<h2 id="4-LLMs什么时候最容易产生幻觉？"><a href="#4-LLMs什么时候最容易产生幻觉？" class="headerlink" title="4. LLMs什么时候最容易产生幻觉？"></a>4. LLMs什么时候最容易产生幻觉？</h2><ol>
<li>数据</li>
<li>长文本</li>
<li>推理能力不够</li>
<li>上下文与预训练知识冲突</li>
<li>错误的上下文</li>
</ol>
<h1 id="6-强化学习-RLHF人类反馈强化学习"><a href="#6-强化学习-RLHF人类反馈强化学习" class="headerlink" title="6. 强化学习-RLHF人类反馈强化学习"></a>6. 强化学习-RLHF人类反馈强化学习</h1><h2 id="1-介绍一下-LLM的经典预训练Pipeline？"><a href="#1-介绍一下-LLM的经典预训练Pipeline？" class="headerlink" title="1. 介绍一下 LLM的经典预训练Pipeline？"></a>1. 介绍一下 LLM的经典预训练Pipeline？</h2><ol>
<li>预训练</li>
<li>有监督微调</li>
<li>对齐</li>
</ol>
<h2 id="2-具体介绍一下预训练？"><a href="#2-具体介绍一下预训练？" class="headerlink" title="2. 具体介绍一下预训练？"></a>2. 具体介绍一下预训练？</h2><p>用数十亿到数万亿个token的庞大文本语料库对模型预训练，使模型能够根据提供的文本来预测下一个单词。</p>
<h2 id="3-具体介绍一下有监督微调？"><a href="#3-具体介绍一下有监督微调？" class="headerlink" title="3. 具体介绍一下有监督微调？"></a>3. 具体介绍一下有监督微调？</h2><p>虽然 SFT 训练目标和 预训练（Pre-training）类似，也是 需要模型预测下一个单词，但是需要人工标注的指令数据集，其中模型的输入是一个指令（根据任务的不同，也可能包含一段输入文本），输出为模型的预期回复内容。</p>
<h2 id="4-简单介绍一下对齐？"><a href="#4-简单介绍一下对齐？" class="headerlink" title="4. 简单介绍一下对齐？"></a>4. 简单介绍一下对齐？</h2><p>通过微调的方式，将语言模型与人类的偏好、价值观进行对齐，这也是RLHF机制发挥的地方。</p>
<h2 id="5-简单介绍一下-RLHF-流程？"><a href="#5-简单介绍一下-RLHF-流程？" class="headerlink" title="5. 简单介绍一下 RLHF 流程？"></a>5. 简单介绍一下 RLHF 流程？</h2><ol>
<li>对预训练之后的模型进行有监督微调</li>
<li>有监督微调上创建一个reward model模型</li>
<li>使用<strong>PPO</strong>算法微调SFT模型</li>
</ol>
<h2 id="6-如何在有监督微调模型基础上创建一个RM模型？"><a href="#6-如何在有监督微调模型基础上创建一个RM模型？" class="headerlink" title="6. 如何在有监督微调模型基础上创建一个RM模型？"></a>6. 如何在有监督微调模型基础上创建一个RM模型？</h2><p>对于每个Prompt，要求有监督微调后的LLM生成四到九个回复，再由标注人员根据个人偏好对所有回复进行排序。在处理排序数据时，SFT的输出通过一个回归层（单个输出节点）转换为奖励分数，即可称为RM模型。</p>
<h2 id="7-大语言模型RLHF中的PPO主要分哪些步骤？"><a href="#7-大语言模型RLHF中的PPO主要分哪些步骤？" class="headerlink" title="7. 大语言模型RLHF中的PPO主要分哪些步骤？"></a>7. 大语言模型RLHF中的PPO主要分哪些步骤？</h2><ol>
<li>采样</li>
<li>反馈</li>
<li>学习</li>
</ol>
<h2 id="8-举例描述一下大语言模型的RLHF？"><a href="#8-举例描述一下大语言模型的RLHF？" class="headerlink" title="8. 举例描述一下大语言模型的RLHF？"></a>8. 举例描述一下大语言模型的RLHF？</h2><p>大语言模型的RLHF 好比是：老师与学生的角色</p>
<ol>
<li>我们扮演着老师的角色，给出有趣的问题。模型则会像小学生一样，不断尝试给出答案。</li>
<li>模型会根据我们给出的问题，写出它觉得正确的答案，但是这些答案不一定是真的答案，需要我们结合正确答案进行打分。如果它表现得好，就会给予它高声赞扬；如果它表现不佳，我们则会给予它耐心的指导和反馈，帮助它不断改进，直到达到令人满意的水平。</li>
</ol>
<h2 id="8-PPO采样过程"><a href="#8-PPO采样过程" class="headerlink" title="8. PPO采样过程"></a>8. PPO采样过程</h2><p>PPO 中采样过程：学生回答问题的过程，是模型根据提示（prompt）输出回答（response）的过程</p>
<h2 id="9-介绍一下-PPO-中-采样策略？"><a href="#9-介绍一下-PPO-中-采样策略？" class="headerlink" title="9.介绍一下 PPO 中 采样策略？"></a>9.介绍一下 PPO 中 采样策略？</h2><p>由两个模型组成，一个叫做演员模型（Actor），另一个叫做评论家模型（Critic）。它们就像是学生大脑中的两种意识，一个负责决策，一个负责总结得失。</p>
<ol>
<li>演员：输入一段上下文，它将输出下一个token的概率分布。</li>
<li>评论家：强化学习的辅助模型，输入一段上下文，它将输出下一个token的“收益”。</li>
</ol>
<h2 id="10-instructGPT的原理，讲讲rlhf和reward？"><a href="#10-instructGPT的原理，讲讲rlhf和reward？" class="headerlink" title="10. instructGPT的原理，讲讲rlhf和reward？"></a>10. instructGPT的原理，讲讲rlhf和reward？</h2><ol>
<li>instructGPT是一种基于强化学习的文本生成模型，其核心原理涉及两个概念：RLHF和Reward shaping（奖励塑造）</li>
<li>RLHF：<ol>
<li>对预训练之后的模型进行有监督微调</li>
<li>有监督微调上创建一个reward model模型</li>
<li>使用<strong>PPO</strong>算法微调SFT模型</li>
</ol>
</li>
<li>Reward shaping：将人类评估者的反馈与模型生成的文本进行比较，可以计算出一个差异度量，用作奖励信号的一部分。根据这个奖励信号进行训练，进行强化学习的训练。模型生成文本，通过奖励信号来评估生成文本的质量。模型的目标是最大化预期累积奖励，从而生成更高质量的文本。</li>
</ol>
<h1 id="7-训练集面"><a href="#7-训练集面" class="headerlink" title="7. 训练集面"></a>7. 训练集面</h1><h2 id="1-SFT（有监督微调）的数据集格式？"><a href="#1-SFT（有监督微调）的数据集格式？" class="headerlink" title="1. SFT（有监督微调）的数据集格式？"></a>1. SFT（有监督微调）的数据集格式？</h2><p>问答</p>
<h2 id="2-RM（奖励模型）的数据格式？"><a href="#2-RM（奖励模型）的数据格式？" class="headerlink" title="2. RM（奖励模型）的数据格式？"></a>2. RM（奖励模型）的数据格式？</h2><p>一个问题 + 一条好回答样例 + 一条差回答样例</p>
<h1 id="8-推理面"><a href="#8-推理面" class="headerlink" title="8. 推理面"></a>8. 推理面</h1><h2 id="1-如何估算模型所需的RAM？"><a href="#1-如何估算模型所需的RAM？" class="headerlink" title="1. 如何估算模型所需的RAM？"></a>1. 如何估算模型所需的RAM？</h2><p>int8 的 LLaMA-6B</p>
<ol>
<li>模型参数:6B*1byte&#x3D;6GB</li>
<li>梯度：6GB</li>
<li>优化器参数：AdamW：6GB*2&#x3D;12GB</li>
<li>cuda kernel：1.3GB</li>
</ol>
<p>25.3GB</p>
<h1 id="9-微调面"><a href="#9-微调面" class="headerlink" title="9. 微调面"></a>9. 微调面</h1><blockquote>
<p><strong>补：</strong></p>
<p><strong>1.微调方法是啥？如何微调？</strong></p>
<ol>
<li>fine-tune，也叫全参微调，bert微调模型一直用的这种方法，全部参数权重参与更新以适配领域数据，效果好。</li>
<li>prompt-tune, 包括p-tuning、lora、prompt-tuning、adaLoRA等delta tuning方法，部分模型参数参与微调，训练快，显存占用少，效果可能跟FT（fine-tune）比会稍有效果损失，但一般效果能打平。</li>
</ol>
<p><strong>2.介绍一下 PEFT？</strong></p>
<p>参数高效微调（Parameter-Efficient Fine-Tuning, PEFT），PEFT技术旨在通过最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。</p>
</blockquote>
<h2 id="1-领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？"><a href="#1-领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？" class="headerlink" title="1. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？"></a>1. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？</h2><ol>
<li>动机：仅仅使用领域数据集进行模型训练，模型很容易出现灾难性遗忘现象.</li>
<li>解决方法：通常在领域训练的过程中加入通用数据集。当数据量没有那么多时，一般领域数据与通用数据的比例在1:5到1:10之间是比较合适的。</li>
</ol>
<h2 id="2-微调后的模型出现能力劣化，灾难性遗忘是怎么回事？"><a href="#2-微调后的模型出现能力劣化，灾难性遗忘是怎么回事？" class="headerlink" title="2.  微调后的模型出现能力劣化，灾难性遗忘是怎么回事？"></a>2.  微调后的模型出现能力劣化，灾难性遗忘是怎么回事？</h2><p>所谓的灾难性遗忘：即学习了新的知识之后，几乎彻底遗忘掉之前习得的内容。模型是走完 “预训练-SFT-RLHF” 过程训练后的模型，其SFT阶段已经有上千指令微调任务训练过，现在我们只是新增了一类指令数据，相对大模型而已，微调数据量少和微调任务类型单一，不会对其原有的能力造成大的影响，所以我认为是不会导致灾难性遗忘问题。应该是微调训练参数调整导致的，微调初始学习率不要设置太高，lr&#x3D;2e-5或者更小，可以避免此问题，不要大于预训练时的学习率。</p>
<h2 id="3-微调大模型时，如果-batch-size-设置太小-会出现什么问题？"><a href="#3-微调大模型时，如果-batch-size-设置太小-会出现什么问题？" class="headerlink" title="3. 微调大模型时，如果 batch size 设置太小 会出现什么问题？"></a>3. 微调大模型时，如果 batch size 设置太小 会出现什么问题？</h2><p>当 batch size 较小时，更新方向（即对真实梯度的近似）会具有很高的方差，导致的梯度更新主要是噪声。经过一些更新后，方差会相互抵消，总体上推动模型朝着正确的方向前进，但个别更新可能不太有用，可以一次性应用（使用更大 batch size 进行更新）。</p>
<h2 id="4-微调大模型时，如果-batch-size-设置太大-会出现什么问题？"><a href="#4-微调大模型时，如果-batch-size-设置太大-会出现什么问题？" class="headerlink" title="4.  微调大模型时，如果 batch size 设置太大 会出现什么问题？"></a>4.  微调大模型时，如果 batch size 设置太大 会出现什么问题？</h2><p>当 batch size 非常大时，我们从训练数据中抽样的任何两组数据都会非常相似（因为它们几乎完全匹配真实梯度）。因此，在这种情况下，增加 batch size 几乎不会改善性能，因为你无法改进真实的梯度预测。换句话说，你需要在每一步中处理更多的数据，但并不能减少整个训练过程中的步数，这表明总体训练时间几乎没有改善。但是更糟糕的是你增加了总体的 FLOPS。</p>
<h2 id="5-微调大模型时-优化器如何？"><a href="#5-微调大模型时-优化器如何？" class="headerlink" title="5.  微调大模型时, 优化器如何？"></a>5.  微调大模型时, 优化器如何？</h2><p>除了Adam和AdamW，其他优化器如Sophia也值得研究，它使用梯度曲率而非方差进行归一化，可能提高训练效率和模型性能。</p>
<h2 id="6-大模型训练loss突刺是什么？"><a href="#6-大模型训练loss突刺是什么？" class="headerlink" title="6. 大模型训练loss突刺是什么？"></a>6. 大模型训练loss突刺是什么？</h2><p>loss spike指的是预训练过程中，尤其容易在大模型（100B以上）预训练过程中出现的loss突然暴涨的情况</p>
<h2 id="7-为什么大模型训练会出现loss突刺？"><a href="#7-为什么大模型训练会出现loss突刺？" class="headerlink" title="7. 为什么大模型训练会出现loss突刺？"></a>7. 为什么大模型训练会出现loss突刺？</h2><p>loss spike的出现和浅层的梯度更新幅度密切相关，实际上就是浅层网络参数突然进入到了之前长时间不在的状态，与模型深层参数当前的状态形成了连锁反应造成了模型进入非稳态。同时一般情况即使出现loss spike也会自动回复到正常状态，但也有可能再也不会</p>
<h2 id="8-大模型训练loss突刺-如何解决？"><a href="#8-大模型训练loss突刺-如何解决？" class="headerlink" title="8. 大模型训练loss突刺 如何解决？"></a>8. 大模型训练loss突刺 如何解决？</h2><ol>
<li><p>出现loss spike后更换batch样本</p>
</li>
<li><p>减小learning rate</p>
</li>
</ol>
<h1 id="10-LoRA面"><a href="#10-LoRA面" class="headerlink" title="10. LoRA面"></a>10. LoRA面</h1><blockquote>
<p><strong>补：</strong></p>
<p><strong>1.为什么 需要 适配器微调（Adapter-tuning）？</strong></p>
<p>预训练模型参数量变多，在特定任务下进行全量微调即昂贵又耗时；</p>
<p><strong>2.适配器微调（Adapter-tuning）思路？</strong></p>
<ol>
<li>设计了 Adapter 结构（首先是一个 down-project 层将高维度特征映射到低维特征，然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征；同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为 identity），并将其嵌入Transformer 的结构里面；</li>
<li>在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数）。</li>
</ol>
<p><strong>3.适配器微调（Adapter-tuning）特点是什么？</strong><br>通过在 Transformer 层中嵌入 Adapter 结构，在推理时会额外增加推理时长。<br><strong>4.AdapterFusion 思路 是什么？</strong><br>一种融合多任务信息的 Adapter 的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。<br><strong>5.AdapterDrop 思路 是什么？</strong><br>在不影响任务性能的情况下，对 Adapter 动态高效的移除，尽可能的减少模型的参数量，提高模型在反向传播（训练）和正向传播（推理）时的效率。<br><strong>6.AdapterDrop 特点 是什么？</strong><br>通过从较低的 Transformer 层删除可变数量的 Adaper 来提升推理速度；当对多个任务执行推理时，动态地减少了运行时的计算开销，并在很大程度上保持了任务性能。<br><strong>7.MAM Adapter 思路 是什么？</strong><br>一种在 Adapter、Prefix Tuning 和 LoRA 之间建立联系的统一方法。最终的模型MAM Adapter 是用于 FFN 的并行 Adapter 和 软提示的组合。<br><strong>8.MAM Adapter 特点 是什么？</strong><br>整体上来说，最终的模型 MAM Adapter 效果会优于单个高效微调方法。</p>
</blockquote>
<h2 id="1-什么是-LoRA？"><a href="#1-什么是-LoRA？" class="headerlink" title="1.  什么是 LoRA？"></a>1.  什么是 LoRA？</h2><p>通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练</p>
<h2 id="2-LoRA-的思路是什么？"><a href="#2-LoRA-的思路是什么？" class="headerlink" title="2. LoRA 的思路是什么？"></a>2. LoRA 的思路是什么？</h2><ol>
<li>在原模型旁边增加一个旁路，通过低秩分解（先降维再升维）来模拟参数的更新量；</li>
<li>训练时，原模型固定，只训练降维矩阵A和升维矩阵B；</li>
<li>推理时，可将BA加到原参数上，不引入额外的推理延迟；</li>
<li>初始化，A采用高斯分布初始化，B初始化为全0，保证训练开始时旁路为0矩阵；</li>
</ol>
<h2 id="3-LoRA-的特点是什么？"><a href="#3-LoRA-的特点是什么？" class="headerlink" title="3. LoRA 的特点是什么？"></a>3. LoRA 的特点是什么？</h2><ol>
<li>将BA加到W上可以消除推理延迟；</li>
<li>可以通过可插拔的形式切换到不同的任务；</li>
<li>简单且效果好；</li>
</ol>
<h2 id="4-QLoRA-的思路是怎么样的？"><a href="#4-QLoRA-的思路是怎么样的？" class="headerlink" title="4. QLoRA 的思路是怎么样的？"></a>4. QLoRA 的思路是怎么样的？</h2><ol>
<li>使用一种新颖的高精度技术将预训练模型量化为 4 bit；</li>
<li>添加一组可学习的低秩适配器权重，通过量化权重的反向传播梯度进行微调。</li>
<li>使用 QLoRA 微调模型，可以显著降低对于显存的要求。同时，模型训练的速度会慢于LoRA。</li>
</ol>
<h2 id="5-AdaLoRA-的思路是怎么样的？"><a href="#5-AdaLoRA-的思路是怎么样的？" class="headerlink" title="5. AdaLoRA 的思路是怎么样的？"></a>5. AdaLoRA 的思路是怎么样的？</h2><p>它根据重要性评分动态分配参数预算给权重矩阵，将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。</p>
<h2 id="6-LoRA微调方法为啥能加速训练？"><a href="#6-LoRA微调方法为啥能加速训练？" class="headerlink" title="6. LoRA微调方法为啥能加速训练？"></a>6. LoRA微调方法为啥能加速训练？</h2><ol>
<li>只更新了部分参数：比如LoRA原论文就选择只更新Self Attention的参数，实际使用时我们还可以选择只更新部分层的参数；</li>
<li>减少了通信时间：由于更新的参数量变少了，所以（尤其是多卡训练时）要传输的数据量也变少了，从而减少了传输时间；</li>
<li>采用了各种低精度加速技术，如FP16、FP8或者INT8量化等。</li>
</ol>
<h2 id="7-如何在已有LoRA模型上继续训练？"><a href="#7-如何在已有LoRA模型上继续训练？" class="headerlink" title="7.  如何在已有LoRA模型上继续训练？"></a>7.  如何在已有LoRA模型上继续训练？</h2><p>把之前的LoRA跟base model 合并后，继续训练就可以，为了保留之前的知识和能力，训练新的LoRA时，加入一些之前的训练数据是需要的。</p>
<h2 id="8-LoRA-缺点是什么？"><a href="#8-LoRA-缺点是什么？" class="headerlink" title="8. LoRA 缺点是什么？"></a>8. LoRA 缺点是什么？</h2><p>参与训练的模型参数量不多，也就百万到千万级别的参数量，所以效果比全量微调差很多。可能在扩散模型上感知没那么强，但在LLM上，个人感觉表现还是差距挺大的。</p>
<h2 id="9-Rank-如何选取？"><a href="#9-Rank-如何选取？" class="headerlink" title="9. Rank 如何选取？"></a>9. Rank 如何选取？</h2><p>Rank的取值作者对比了1-64，效果上Rank在4-8之间最好，再高并没有效果提升。不过论文的实验是面向下游单一监督任务的，因此在指令微调上根据指令分布的广度，Rank选择还是需要在8以上的取值进行测试。</p>
<h2 id="10-alpha参数-如何选取？"><a href="#10-alpha参数-如何选取？" class="headerlink" title="10. alpha参数 如何选取？"></a>10. alpha参数 如何选取？</h2><p>alpha其实是个缩放参数，本质和learning rate相同，所以为了简化我默认让alpha&#x3D;rank，只调整lr，这样可以简化超参。</p>
<h2 id="11-LoRA-高效微调-如何避免过拟合？"><a href="#11-LoRA-高效微调-如何避免过拟合？" class="headerlink" title="11. LoRA 高效微调 如何避免过拟合？"></a>11. LoRA 高效微调 如何避免过拟合？</h2><p>减小r或增加数据集大小可以帮助减少过拟合。还可以尝试增加优化器的权重衰减率或LoRA层的dropout值。</p>
<h2 id="12-Lora的矩阵怎么初始化？为什么？"><a href="#12-Lora的矩阵怎么初始化？为什么？" class="headerlink" title="12. Lora的矩阵怎么初始化？为什么？"></a>12. Lora的矩阵怎么初始化？为什么？</h2><ol>
<li>矩阵B被初始化为0，而矩阵A正常高斯初始化</li>
<li>如果B，A全都初始化为0，那么缺点与深度网络全0初始化一样，很容易导致梯度消失(因为此时初始所有神经元的功能都是等价的)。</li>
<li>如果B，A全部高斯初始化，那么在网络训练刚开始就会有概率为得到一个过大的偏移值Δ W 从而引入太多噪声，导致难以收敛</li>
<li>因此，一部分初始为0，一部分正常初始化是为了在训练开始时维持网络的原有输出(初始偏移为0)，但同时也保证在真正开始学习后能够更好的收敛。</li>
</ol>
<h1 id="11-提示学习"><a href="#11-提示学习" class="headerlink" title="11. 提示学习"></a>11. 提示学习</h1><h2 id="1-什么是-提示学习（Prompting）？"><a href="#1-什么是-提示学习（Prompting）？" class="headerlink" title="1. 什么是 提示学习（Prompting）？"></a>1. 什么是 提示学习（Prompting）？</h2><p>Prompt 提供上下文和任务相关信息，以帮助模型更好地理解要求，并生成正确的输出。</p>
<h2 id="2-为什么需要-提示学习（Prompting）？"><a href="#2-为什么需要-提示学习（Prompting）？" class="headerlink" title="2. 为什么需要 提示学习（Prompting）？"></a>2. 为什么需要 提示学习（Prompting）？</h2><ol>
<li>在面对特定的下游任务时，如果进行 Full FineTuning（即对预训练模型中的所有参数都进行微调），太过低效；</li>
<li>而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</li>
</ol>
<h2 id="3-提示学习（Prompting）-有什么优点？"><a href="#3-提示学习（Prompting）-有什么优点？" class="headerlink" title="3. 提示学习（Prompting） 有什么优点？"></a>3. 提示学习（Prompting） 有什么优点？</h2><ol>
<li>最小化微调参数的数量和计算复杂度，来提高预训练模型在新任务上的性能，从而缓解大型预训练模型的训练成本。</li>
<li>这样一来，即使计算资源受限，也可以利用预训练模型的知识来迅速适应新任务，实现高效的迁移学习。</li>
</ol>
<h2 id="4-为什么需要-前缀微调（Prefix-tining）？"><a href="#4-为什么需要-前缀微调（Prefix-tining）？" class="headerlink" title="4. 为什么需要 前缀微调（Prefix-tining）？"></a>4. 为什么需要 前缀微调（Prefix-tining）？</h2><ol>
<li>人工设计离散的 Prompts 缺点：Prompts 的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化</li>
<li>自动化搜索离散的 Prompts 缺点：成本也比较高，离散化的 token 搜索出来的结果可能并不是最优的；</li>
<li>传统的微调范式利用预训练模型去对不同的下游任务进行微调，对每个任务都要保存一份微调后的模型权重，一方面微调整个模型耗时长；另一方面也会占很多存储空间</li>
</ol>
<h2 id="5-前缀微调（Prefix-tining）思路是什么？"><a href="#5-前缀微调（Prefix-tining）思路是什么？" class="headerlink" title="5. 前缀微调（Prefix-tining）思路是什么？"></a>5. 前缀微调（Prefix-tining）思路是什么？</h2><ol>
<li>Prefix 构建：在输入 token 之前构造一段任务相关的可学习的虚拟令牌 virtual tokens 作为 Prefix；</li>
<li>训练时只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定；</li>
<li>在 Prefix 层前面加了 MLP 结构(相当于将 Prefix 分解为更小维度的 Input 与 MLP 的组合后输出的结果)，训练完成后，只保留 Prefix 的参数；（用于 防止直接更新 Prefix 的参数导致训练不稳定的情况）</li>
</ol>
<h2 id="6-前缀微调（Prefix-tining）的优点是什么？"><a href="#6-前缀微调（Prefix-tining）的优点是什么？" class="headerlink" title="6. 前缀微调（Prefix-tining）的优点是什么？"></a>6. 前缀微调（Prefix-tining）的优点是什么？</h2><ol>
<li>相比人工设计离散的 Prompts ：前缀微调（Prefixtining） 可以学习的“隐式”的 Prompts；</li>
<li>相比full fine-tuning：full fine-tuning 更新所有参数，Prefix Tuning 只更新 Prefix 部分的参数；</li>
<li>基于前缀的架构可以在一个批次中处理来自多个用户&#x2F;任务的样本，这是其他轻量级微调方法所不能做到的；</li>
</ol>
<h2 id="7-前缀微调（Prefix-tining）的缺点是什么？"><a href="#7-前缀微调（Prefix-tining）的缺点是什么？" class="headerlink" title="7.  前缀微调（Prefix-tining）的缺点是什么？"></a>7.  前缀微调（Prefix-tining）的缺点是什么？</h2><ol>
<li>占用序列长度。有一定的额外计算开销;</li>
<li>在每层都加了 prompt 的参数，改动较大</li>
</ol>
<h2 id="8-为什么需要-指示微调（Prompt-tuning）？"><a href="#8-为什么需要-指示微调（Prompt-tuning）？" class="headerlink" title="8. 为什么需要 指示微调（Prompt-tuning）？"></a>8. 为什么需要 指示微调（Prompt-tuning）？</h2><ol>
<li>模型全量微调对每个任务训练一个模型，开销和部署成本都比较高；</li>
<li>离散的 prompts（指人工设计 prompts 提示语加入到模型）方法，成本比较高，并且效果不太好；</li>
<li>前缀微调（Prefix-tining）占用序列长度。有一定的额外计算开销;</li>
<li>前缀微调（Prefix-tining）在每层都加了 prompt 的参数，改动较大;</li>
</ol>
<h2 id="9-指示微调（Prompt-tuning）思路是什么？"><a href="#9-指示微调（Prompt-tuning）思路是什么？" class="headerlink" title="9. 指示微调（Prompt-tuning）思路是什么？"></a>9. 指示微调（Prompt-tuning）思路是什么？</h2><ol>
<li>将 prompt 扩展到连续空间，仅在 输入层 添加 prompt 连续向量，通过反向传播更新参数来学习 prompts，而不是人工设计 prompts；</li>
<li>冻结模型原始权重，只训练 prompts 参数，训练完成后，只用同一个模型可以做多任务推理；</li>
<li>使用 LSTM 建模 prompt 向量间 关联性</li>
</ol>
<h2 id="10-指示微调（Prompt-tuning）优点是什么？"><a href="#10-指示微调（Prompt-tuning）优点是什么？" class="headerlink" title="10. 指示微调（Prompt-tuning）优点是什么？"></a>10. 指示微调（Prompt-tuning）优点是什么？</h2><ol>
<li>只在输入层加入 prompt tokens，并且不需要加入 MLP 进行调整来解决难训练的问题；</li>
<li>随着预训练模型参数量的增加，Prompt Tuning 的方法会逼近全参数微调的结果;</li>
<li>提出了 prompt ensembling：在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方式询问同一个问题），这样相当于训练了不同模型，比模型集成的成本小多了；</li>
</ol>
<h2 id="11-指示微调（Prompt-tuning）缺点是什么？"><a href="#11-指示微调（Prompt-tuning）缺点是什么？" class="headerlink" title="11. 指示微调（Prompt-tuning）缺点是什么？"></a>11. 指示微调（Prompt-tuning）缺点是什么？</h2><ol>
<li>训练难度加大。不太好训练，省了显存，但不一定省时间。具体来讲，大部分 prompt 现在只是 parameter efficient 并没有达到想要的 training efficient。也就是说只是省了空间(显存)，但不一定能加快训练，训练时间有可能更长。</li>
<li>多个 prompt token 之间相互独立，可能会影响效果</li>
<li>在 NLU 上，prompt tuning 对于正常大小的预训练模型表现不佳；</li>
<li>现有的 prompt tuning 方法不能处理困难的序列标注任务</li>
</ol>
<h2 id="12-指示微调（Prompt-tuning）与-Prefix-tuning-区别-是什么？"><a href="#12-指示微调（Prompt-tuning）与-Prefix-tuning-区别-是什么？" class="headerlink" title="12.  指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？"></a>12.  指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？</h2><ol>
<li>可以看作是 Prefix Tuning 的简化版本</li>
<li>适用任务不同<ol>
<li>Prefix-tuning 仅针对 NLG 任务有效，服务于 GPT 架构；</li>
<li>指示微调（Prompt-tuning） 考虑所有类型的语言模型</li>
</ol>
</li>
<li>添加方式不同<ol>
<li>Prefix-tuning 限定在输入前面添加</li>
<li>指示微调（Prompt-tuning） 可以在任意位置添加</li>
</ol>
</li>
<li>prompt 连续向量添加方式不同<ol>
<li>Prefix-tuning 每一层都添加，保证效果</li>
<li>指示微调（Prompt-tuning） 可以只在 输入层 添加</li>
</ol>
</li>
</ol>
<h2 id="13-指示微调（Prompt-tuning）与-fine-tuning-区别-是什么？"><a href="#13-指示微调（Prompt-tuning）与-fine-tuning-区别-是什么？" class="headerlink" title="13. 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？"></a>13. 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？</h2><ol>
<li>Fine-tuning 需要改变预训练阶段模型参数， 可能带量灾难性遗忘问题</li>
<li>指示微调（Prompt-tuning） 不改变预训练阶段模型参数，而是通过微调寻找更好的连续prompt，来引导已学习到的知识使用</li>
</ol>
<h2 id="14-为什么需要-P-tuning？"><a href="#14-为什么需要-P-tuning？" class="headerlink" title="14. 为什么需要 P-tuning？"></a>14. 为什么需要 P-tuning？</h2><ol>
<li>大模型的 Prompt 构造方式严重影响下游任务的效果。</li>
<li>人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化</li>
<li>自动化搜索模版工作成本也比较高，以前这种离散化的 token 的搜索出来的结果可能并不是最优的，导致性能不稳定；</li>
</ol>
<h2 id="15-P-tuning-思路是什么？"><a href="#15-P-tuning-思路是什么？" class="headerlink" title="15. P-tuning 思路是什么？"></a>15. P-tuning 思路是什么？</h2><ol>
<li>可学习的 Embedding 层 设计。将 Prompt 转换为 可学习 Embedding 层；</li>
<li>prompt encoder 设计。用 prompt encoder（由一个双向的 LSTM+两层 MLP 组成） 的方式来对 Prompt Embedding 进行一层处理，建模伪 token 的相互依赖，并且可以提供一个更好的初始化。</li>
</ol>
<h2 id="16-P-tuning-优点是什么？"><a href="#16-P-tuning-优点是什么？" class="headerlink" title="16. P-tuning 优点是什么？"></a>16. P-tuning 优点是什么？</h2><p>引入 prompt encoder（由一个双向的 LSTM+两层 MLP 组成）来建模伪 token 的相互依赖，并且可以提供一个更好的初始化;</p>
<h2 id="17-P-tuning-缺点是什么？"><a href="#17-P-tuning-缺点是什么？" class="headerlink" title="17. P-tuning 缺点是什么？"></a>17. P-tuning 缺点是什么？</h2><ol>
<li>复杂性增加。稍显复杂，看着不太像 prompt 了；</li>
<li>伪 token 编码时是连续的，但在与输入结合时可能是不连续的，中间可能会插入输入</li>
</ol>
<h2 id="18-为什么需要-P-tuning-v2？"><a href="#18-为什么需要-P-tuning-v2？" class="headerlink" title="18. 为什么需要 P-tuning v2？"></a>18. 为什么需要 P-tuning v2？</h2><p>如何 让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果；</p>
<h2 id="19-P-tuning-v2-思路是什么？"><a href="#19-P-tuning-v2-思路是什么？" class="headerlink" title="19. P-tuning v2 思路是什么？"></a>19. P-tuning v2 思路是什么？</h2><ol>
<li>Deep Prompt Encoding：采用 Prefix-tuning 的做法，在输入前面的每层加入可微调的Prompts tokens 作为输入；</li>
<li>移除了重参数化的编码器（prefix-tuning 中可选的 MLP、p-tuning 中的 LSTM）：prefixtuning 和 p-tuning，通过利用重参数化功能来提高训练速度和鲁棒性，但是 该方法对于较小的模型，同时还会影响模型的表现；</li>
<li>针对不同任务采用不同的提示长度。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning 中的发现一致，不同的文本生成任务可能有不同的最佳提示长度；</li>
<li>引入多任务学习，先在多任务的 prompt 上进行预训练，然后再适配下游任务；</li>
<li>连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；连续提示是跨任务和数据集的特定任务知识的完美载体；</li>
<li>抛弃了 prompt learing 中常用的 Verbalizer(词表征器或词映射器)，回归到传统的泛系聚类分析（CLS） 和 token label 分类范式。</li>
<li>采用随机初始化的分类头（Classification Head）应用于 tokens 之上，以增强通用性，可以适配到序列标注任务。</li>
</ol>
<h2 id="20-P-tuning-v2-优点是什么？"><a href="#20-P-tuning-v2-优点是什么？" class="headerlink" title="20. P-tuning v2 优点是什么？"></a>20. P-tuning v2 优点是什么？</h2><ol>
<li>在输入前面的每层加入可微调的 Prompts tokens 作为输入，优点：更多可学习的参数（从 P-tuning 和 Prompt Tuning 的 0.01%增加到 0.1%-3%），同时也足够参数高效；</li>
<li>加入到更深层结构中的 Prompt 能给模型预测带来更直接的影响；</li>
<li>解决了 Prompt Tuning 无法在小模型上有效提升的问题；</li>
<li>将 Prompt Tuning 拓展至 NER 等序列标注任务上</li>
</ol>
<h2 id="21-P-tuning-v2-缺点是什么？"><a href="#21-P-tuning-v2-缺点是什么？" class="headerlink" title="21. P-tuning v2 缺点是什么？"></a>21. P-tuning v2 缺点是什么？</h2><p>抛弃了 prompt learing 中常用的 verbalizer，回归到传统的 CLS 和 token label 分类范式，这其实某种程度上弱化了 prompt 的味道</p>
<h1 id="12-langchain面"><a href="#12-langchain面" class="headerlink" title="12. langchain面"></a>12. langchain面</h1><h2 id="1-Components-and-Chains"><a href="#1-Components-and-Chains" class="headerlink" title="1. Components and Chains"></a>1. Components and Chains</h2><p> Components 是模块化单元（如LLMs、记忆模块等），Chains 是将多个组件按特定顺序组合的工作流。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"></span><br><span class="line">llm = OpenAI()</span><br><span class="line">prompt = <span class="string">&quot;写一首关于&#123;theme&#125;的诗&quot;</span></span><br><span class="line">chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt))</span><br><span class="line">chain.run(theme=<span class="string">&quot;春天&quot;</span>)  <span class="comment"># 生成关于春天的诗</span></span><br></pre></td></tr></table></figure>

<h2 id="2-Prompt-Templates-and-Values"><a href="#2-Prompt-Templates-and-Values" class="headerlink" title="2. Prompt Templates and Values"></a>2. Prompt Templates and Values</h2><p> Prompt Templates 是带变量的提示模板，Values 是填充的具体值。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">template = <span class="string">&quot;列举&#123;product&#125;的三个优点&quot;</span></span><br><span class="line">prompt = PromptTemplate(template=template, input_variables=[<span class="string">&quot;product&quot;</span>])</span><br><span class="line">filled_prompt = prompt.<span class="built_in">format</span>(product=<span class="string">&quot;智能手机&quot;</span>) </span><br><span class="line"><span class="comment"># 输出：&quot;列举智能手机的三个优点&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="3-Example-Selectors"><a href="#3-Example-Selectors" class="headerlink" title="3. Example Selectors"></a>3. Example Selectors</h2><p> 动态选择最相关的示例注入提示中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.prompts.example_selector <span class="keyword">import</span> SemanticSimilarityExampleSelector</span><br><span class="line"></span><br><span class="line">examples = [&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;开心&quot;</span>, <span class="string">&quot;output&quot;</span>: <span class="string">&quot;笑容&quot;</span>&#125;,...]</span><br><span class="line">selector = SemanticSimilarityExampleSelector.from_examples(</span><br><span class="line">    examples, </span><br><span class="line">    embeddings_model, </span><br><span class="line">    vectorstore</span><br><span class="line">)</span><br><span class="line">selected_examples = selector.select_examples(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;高兴&quot;</span>&#125;)  <span class="comment"># 获取相似示例</span></span><br></pre></td></tr></table></figure>

<h2 id="4-Output-Parsers"><a href="#4-Output-Parsers" class="headerlink" title="4. Output Parsers"></a>4. Output Parsers</h2><p> 将LLM输出结构化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.output_parsers <span class="keyword">import</span> StructuredOutputParser</span><br><span class="line"></span><br><span class="line">schema = &#123;<span class="string">&quot;colors&quot;</span>: [<span class="string">&quot;str&quot;</span>, <span class="string">&quot;喜欢的颜色&quot;</span>]&#125;</span><br><span class="line">parser = StructuredOutputParser.from_response_schemas(schema)</span><br><span class="line">output = parser.parse(<span class="string">&quot;黄色, 蓝色&quot;</span>)  <span class="comment"># 返回结构化字典</span></span><br></pre></td></tr></table></figure>

<h2 id="5-Indexes-and-Retrievers"><a href="#5-Indexes-and-Retrievers" class="headerlink" title="5. Indexes and Retrievers"></a>5. Indexes and Retrievers</h2><p> Indexes 是文档存储结构，Retrievers 是查询接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> WebBaseLoader</span><br><span class="line"><span class="keyword">from</span> langchain.indexes <span class="keyword">import</span> VectorstoreIndexCreator</span><br><span class="line"></span><br><span class="line">loader = WebBaseLoader(<span class="string">&quot;https://example.com&quot;</span>)</span><br><span class="line">index = VectorstoreIndexCreator().from_loaders([loader])</span><br><span class="line">retriever = index.vectorstore.as_retriever()</span><br><span class="line">docs = retriever.get_relevant_documents(<span class="string">&quot;查询问题&quot;</span>)  <span class="comment"># 获取相关文档</span></span><br></pre></td></tr></table></figure>

<h2 id="6-Chat-Message-History"><a href="#6-Chat-Message-History" class="headerlink" title="6. Chat Message History"></a>6. Chat Message History</h2><p> 维护对话上下文：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ChatMessageHistory</span><br><span class="line"></span><br><span class="line">history = ChatMessageHistory()</span><br><span class="line">history.add_user_message(<span class="string">&quot;你好！&quot;</span>)</span><br><span class="line">history.add_ai_message(<span class="string">&quot;你好，有什么可以帮忙？&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(history.messages)  <span class="comment"># 显示完整对话记录</span></span><br></pre></td></tr></table></figure>

<h2 id="7-Agents-and-Toolkits"><a href="#7-Agents-and-Toolkits" class="headerlink" title="7. Agents and Toolkits"></a>7. Agents and Toolkits</h2><p> Agents 是自主决策系统，Toolkits 是工具集合：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> initialize_agent, Tool</span><br><span class="line"><span class="keyword">from</span> langchain.tools <span class="keyword">import</span> WikipediaQueryRun</span><br><span class="line"></span><br><span class="line">wikipedia_tool = Tool(</span><br><span class="line">    name=<span class="string">&quot;Wikipedia&quot;</span>,</span><br><span class="line">    func=WikipediaQueryRun().run,</span><br><span class="line">    description=<span class="string">&quot;查询百科知识&quot;</span></span><br><span class="line">)</span><br><span class="line">agent = initialize_agent([wikipedia_tool], llm, agent=<span class="string">&quot;zero-shot-react-description&quot;</span>)</span><br><span class="line">agent.run(<span class="string">&quot;爱因斯坦的出生日期是什么时候？&quot;</span>)  <span class="comment"># 自动调用Wikipedia工具查询</span></span><br></pre></td></tr></table></figure>

<h2 id="8-LangChain-如何Embedding-vector-store？"><a href="#8-LangChain-如何Embedding-vector-store？" class="headerlink" title="8. LangChain 如何Embedding &amp; vector store？"></a>8. LangChain 如何Embedding &amp; vector store？</h2><ol>
<li>要在LangChain中进行嵌入和向量存储，您可以使用LangChain框架提供的 Embedding 和VectorStore 类。 <ol>
<li>Embedding 类用于将文本嵌入到向量空间中</li>
<li>VectorStore 类用于存储和检索嵌入向量。</li>
</ol>
</li>
<li>首先创建了一个 Embedding 实例，并使用 embed 方法将文本嵌入到向量空间中。然后，创建了一个 VectorStore 实例，并使用 store 方法将嵌入向量存储到向量存储中。最后，我们使用 retrieve 方法检索嵌入向量，并打印出来。</li>
</ol>
<h1 id="13-Tokenizer面"><a href="#13-Tokenizer面" class="headerlink" title="13. Tokenizer面"></a>13. Tokenizer面</h1><h2 id="1-Tokenizer-介绍"><a href="#1-Tokenizer-介绍" class="headerlink" title="1. Tokenizer 介绍"></a>1. Tokenizer 介绍</h2><p>Tokenizer是一个用于向量化文本，将文本转换为序列的类。计算机在处理语言文字时，是无法理解文字含义的，通常会把一个词（中文单个字或者词）转化为一个正整数，将一个文本就变成了一个序列，然后再对序列进行向量化，向量化后的数据送入模型处理。</p>
<h2 id="2-Byte-Pair-Encoding-BPE"><a href="#2-Byte-Pair-Encoding-BPE" class="headerlink" title="2. Byte-Pair Encoding(BPE)"></a>2. Byte-Pair Encoding(BPE)</h2><ol>
<li>准备足够的训练语料;以及期望的词表大小；</li>
<li>将单词拆分为字符粒度(字粒度)，并在末尾添加后缀“”，统计单词频率</li>
<li>合并方式:统计每一个连续&#x2F;相邻字节对的出现频率，将最高频的连续字节对合并为新字词；</li>
<li>重复第 3 步，直到词表达到设定的词表大小;或下一个最高频字节对出现频率为 1。</li>
<li>注：GPT2、BART 和 LLaMA 就采用了 BPE。</li>
</ol>
<h2 id="3-WordPiece-与-BPE-异同点是什么？"><a href="#3-WordPiece-与-BPE-异同点是什么？" class="headerlink" title="3. WordPiece 与 BPE 异同点是什么？"></a>3. WordPiece 与 BPE 异同点是什么？</h2><p>如何选择两个子词进行合并：</p>
<ol>
<li>BPE 是选择频次最大的相邻子词合并；</li>
<li>WordPiece 算法选择能够提升语言模型概率最大的相邻子词进行合并，来加入词表；</li>
<li>注：BERT 采用了 WordPiece。</li>
</ol>
<h2 id="4-简单介绍一下-SentencePiece-思路？"><a href="#4-简单介绍一下-SentencePiece-思路？" class="headerlink" title="4. 简单介绍一下 SentencePiece 思路？"></a>4. 简单介绍一下 SentencePiece 思路？</h2><ol>
<li>把空格也当作一种特殊字符来处理，再用 BPE 来构造词汇表。</li>
<li>注：ChatGLM、BLOOM、PaLM 采用了 SentencePiece。</li>
</ol>
<h1 id="14-分布式训练面"><a href="#14-分布式训练面" class="headerlink" title="14. 分布式训练面"></a>14. 分布式训练面</h1><h2 id="1-如果有N张显存足够大的显卡，怎么加速训练？"><a href="#1-如果有N张显存足够大的显卡，怎么加速训练？" class="headerlink" title="1. 如果有N张显存足够大的显卡，怎么加速训练？"></a>1. 如果有N张显存足够大的显卡，怎么加速训练？</h2><p>数据并行（DP），充分利用多张显卡的算力。</p>
<h2 id="2-如果显卡的显存不够装下一个完整的模型呢？"><a href="#2-如果显卡的显存不够装下一个完整的模型呢？" class="headerlink" title="2. 如果显卡的显存不够装下一个完整的模型呢？"></a>2. 如果显卡的显存不够装下一个完整的模型呢？</h2><p>分层加载，把不同的层加载到不同的GPU上（accelerate的device_map）也就是常见的PP，流水线并行。</p>
<h2 id="3-PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？"><a href="#3-PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？" class="headerlink" title="3. PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？"></a>3. PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？</h2><ol>
<li>横向切分：流水线并行（PP），也就是分层加载到不同的显卡上。</li>
<li>纵向切分：张量并行（TP），在 DeepSpeed 世界里叫模型并行（MP）</li>
<li>张量并行的显存更省一点，张量并行的通信量实在太高，只能限于节点内（有NVLINK）。如果节点间张量并行，显卡的利用率会低到5%</li>
</ol>
<h2 id="4-3D-并行"><a href="#4-3D-并行" class="headerlink" title="4. 3D 并行"></a>4. 3D 并行</h2><p>DP+PP+TP</p>
<h1 id="15-推理加速面"><a href="#15-推理加速面" class="headerlink" title="15. 推理加速面"></a>15. 推理加速面</h1><h2 id="1-当前优化模型最主要技术手段有哪些？"><a href="#1-当前优化模型最主要技术手段有哪些？" class="headerlink" title="1. 当前优化模型最主要技术手段有哪些？"></a>1. 当前优化模型最主要技术手段有哪些？</h2><ol>
<li>算法层面：蒸馏、量化</li>
<li>软件层面：计算图优化、模型编译</li>
<li>硬件层面：FP8（NVIDIA H系列GPU开始支持FP8，兼有fp16的稳定性和int8的速度）</li>
</ol>
<h2 id="2-推理加速框架有哪一些？都有什么特点？"><a href="#2-推理加速框架有哪一些？都有什么特点？" class="headerlink" title="2. 推理加速框架有哪一些？都有什么特点？"></a>2. 推理加速框架有哪一些？都有什么特点？</h2><ol>
<li>FasterTransformer：英伟达推出的FasterTransformer不修改模型架构而是在计算加速层面优化<br> Transformer 的 encoder 和 decoder 模块。具体包括如下：</li>
<li>尽可能多地融合除了 GEMM 以外的操作</li>
<li>支持 FP16、INT8、FP8</li>
<li>移除 encoder 输入中无用的 padding 来减少计算开销</li>
<li>TurboTransformers：腾讯推出的 TurboTransformers 由 computation runtime 及 serving<br> framework 组成。加速推理框架适用于 CPU 和 GPU，最重要的是，它可以无需预处理便可处理变<br> 长的输入序列。具体包括如下：</li>
<li>与 FasterTransformer 类似，它融合了除 GEMM 之外的操作以减少计算量</li>
<li>smart batching，对于一个 batch 内不同长度的序列，它也最小化了 zero-padding 开销</li>
<li>对 LayerNorm 和 Softmax 进行批处理，使它们更适合并行计算</li>
<li>引入了模型感知分配器，以确保在可变长度请求服务期间内存占用较小</li>
</ol>
<h2 id="3-vLLM-的-功能有哪些？"><a href="#3-vLLM-的-功能有哪些？" class="headerlink" title="3. vLLM 的 功能有哪些？"></a>3. vLLM 的 功能有哪些？</h2><ol>
<li>Continuous batching：有iteration-level的调度机制，每次迭代batch大小都有所变化，因此vLLM<br> 在大量查询下仍可以很好的工作；</li>
<li>PagedAttention：受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的<br> 秘诀</li>
</ol>
<h2 id="4-vLLM-的-优点有哪些？"><a href="#4-vLLM-的-优点有哪些？" class="headerlink" title="4. vLLM 的 优点有哪些？"></a>4. vLLM 的 优点有哪些？</h2><ol>
<li>文本生成的速度：实验多次，发现vLLM的推理速度是最快的；</li>
<li>高吞吐量服务：支持各种解码算法，比如parallel sampling, beam search等；</li>
<li>与OpenAI API兼容：如果使用OpenAI API，只需要替换端点的URL即可；</li>
</ol>
<h2 id="5-vLLM-的-缺点有哪些？"><a href="#5-vLLM-的-缺点有哪些？" class="headerlink" title="5. vLLM 的 缺点有哪些？"></a>5. vLLM 的 缺点有哪些？</h2><ol>
<li>添加自定义模型：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会变得更加复杂。</li>
<li>缺乏对适配器（LoRA、QLoRA等）的支持：当针对特定任务进行微调时，开源LLM具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。</li>
<li>缺少权重量化：有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。</li>
</ol>
<h2 id="6-介绍一下-Text-generation-inference？"><a href="#6-介绍一下-Text-generation-inference？" class="headerlink" title="6. 介绍一下 Text generation inference？"></a>6. 介绍一下 Text generation inference？</h2><p>Text generation inference是用于文本生成推断的Rust、Python和gRPC服务器，在HuggingFace中已有LLM 推理API使用。</p>
<h2 id="7-Text-generation-inference-的-功能有哪些？"><a href="#7-Text-generation-inference-的-功能有哪些？" class="headerlink" title="7. Text generation inference 的 功能有哪些？"></a>7. Text generation inference 的 功能有哪些？</h2><ol>
<li>内置服务评估：可以监控服务器负载并深入了解其性能；</li>
<li>使用flash attention（和v2）和Paged attention优化transformer推理代码：并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；</li>
</ol>
<h2 id="8-Text-generation-inference-的-优点有哪些？"><a href="#8-Text-generation-inference-的-优点有哪些？" class="headerlink" title="8. Text generation inference 的 优点有哪些？"></a>8. Text generation inference 的 优点有哪些？</h2><ol>
<li>所有的依赖项都安装在Docker中：会得到一个现成的环境；</li>
<li>支持HuggingFace模型：轻松运行自己的模型或使用任何HuggingFace模型中心；</li>
<li>对模型推理的控制：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；</li>
</ol>
<h2 id="9-Text-generation-inference-的-缺点有哪些？"><a href="#9-Text-generation-inference-的-缺点有哪些？" class="headerlink" title="9. Text generation inference 的 缺点有哪些？"></a>9. Text generation inference 的 缺点有哪些？</h2><ol>
<li>缺乏对适配器的支持：需要注意的是，尽管可以使用适配器部署LLM，但目前还没有官方支持或文档；</li>
<li>从源代码（Rust+CUDA内核）编译：对于不熟悉Rust的人，将客户化代码纳入库中变得很有挑战性；</li>
<li>文档不完整：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；</li>
</ol>
<h1 id="16-显存问题面"><a href="#16-显存问题面" class="headerlink" title="16. 显存问题面"></a>16. 显存问题面</h1><h2 id="1-nB-模型训练需要多少显存？"><a href="#1-nB-模型训练需要多少显存？" class="headerlink" title="1. nB 模型训练需要多少显存？"></a>1. nB 模型训练需要多少显存？</h2><p>基础显存：模型参数+梯度+优化器，总共 16nG。</p>
<h1 id="17-增量预训练面"><a href="#17-增量预训练面" class="headerlink" title="17. 增量预训练面"></a>17. 增量预训练面</h1><h2 id="1-为什么要增量预训练？"><a href="#1-为什么要增量预训练？" class="headerlink" title="1. 为什么要增量预训练？"></a>1. 为什么要增量预训练？</h2><p>有一种观点，预训练学知识，指令微调学格式，强化学习对齐人类偏好，LIMA 等论文算是这一观点的证据。所以要想大模型有领域知识，得增量预训练。（靠指令微调记知识不靠谱，不是几十 w 条数据能做到的）</p>
<h1 id="18-蒸馏面"><a href="#18-蒸馏面" class="headerlink" title="18. 蒸馏面"></a>18. 蒸馏面</h1><h2 id="1-知识蒸馏和无监督样本训练？"><a href="#1-知识蒸馏和无监督样本训练？" class="headerlink" title="1. 知识蒸馏和无监督样本训练？"></a>1. 知识蒸馏和无监督样本训练？</h2><ol>
<li>知识蒸馏是利用大模型把一个大模型的知识压缩到一个小模型上。具体来说你在一个训练集上得到了一个非常好的较大的模型，然后你把这个模型冻结，作为Teacher模型也叫监督模型，然后你再造一个较小参数的模型叫做Student模型，我们的目标就是利用冻结的Teacher模型去训练Student模型。<ol>
<li>离线蒸馏：Student在训练集上的loss和与Teacher模型的loss作为总的loss，一起优化。</li>
<li>半监督蒸馏：向Teacher模型输入一些input得到标签，然后把input和标签传给Student模型</li>
<li>还有个自监督蒸馏，直接不要Teacher模型，在最后几轮epoch，把前面训练好的模型作为Teacher进行监督。</li>
<li>目前知识蒸馏的一个常见应用就是对齐ChatGPT。</li>
</ol>
</li>
<li>无监督样本训练，我看不懂意思。如果是传统的无监督学习，那就是聚类，主成分分析等操作。如果是指知识蒸馏的话，就是离线蒸馏的方式，只不过损失只有和Teacher的loss。</li>
</ol>
<h2 id="2-对知识蒸馏知道多少，有哪些改进用到了？"><a href="#2-对知识蒸馏知道多少，有哪些改进用到了？" class="headerlink" title="2. 对知识蒸馏知道多少，有哪些改进用到了？"></a>2. 对知识蒸馏知道多少，有哪些改进用到了？</h2><p>知识蒸馏是一种通过将一个复杂模型的知识转移到一个简单模型来提高简单模型性能的方法。这种方法已经被广泛应用于各种深度学习任务中。其中一些改进包括：</p>
<ol>
<li>使用不同类型的损失函数和温度参数来获得更好的知识蒸馏效果。</li>
<li>引入额外的信息来提高蒸馏的效果，例如将相似性约束添加到模型训练中。</li>
<li>将蒸馏方法与其他技术结合使用，例如使用多任务学习和迁移学习来进一步改进知识蒸馏的效果。</li>
</ol>
<h2 id="3-谈一下对模型量化的了解？"><a href="#3-谈一下对模型量化的了解？" class="headerlink" title="3. 谈一下对模型量化的了解？"></a>3. 谈一下对模型量化的了解？</h2><p>模型量化是一种将浮点型参数转换为定点型参数的技术，以减少模型的存储和计算复杂度。常见的模型量化方法包括：</p>
<ol>
<li>量化权重和激活值，将它们转换为整数或小数。</li>
<li>使用更小的数据类型，例如8位整数、16位浮点数等。</li>
<li>使用压缩算法，例如Huffman编码、可逆压缩算法等。</li>
</ol>
<p>模型量化可以减少模型的存储空间和内存占用，同时也可以加速模型的推理速度。但是，模型量化可能会对模型的精度造成一定的影响，因此需要仔细权衡精度和计算效率之间的平衡。</p>
<h2 id="4-模型压缩和加速的方法有哪些？"><a href="#4-模型压缩和加速的方法有哪些？" class="headerlink" title="4. 模型压缩和加速的方法有哪些？"></a>4. 模型压缩和加速的方法有哪些？</h2><ol>
<li><p>剪枝：</p>
<ol>
<li>参数剪枝（Parameter Pruning）：删除模型中冗余的参数，减少模型的大小。通常情况下，只有很少一部分参数对模型的性能贡献较大，其余参数对性能的贡献较小或没有贡献，因此可以删除这些冗余参数。</li>
<li>网络剪枝（Network Pruning）：删除模型中冗余的神经元，从而减小模型的大小。与参数剪枝不同，网络剪枝可以删除神经元而不会删除对应的参数。</li>
<li>层次化剪枝（Layer-wise Pruning）：对模型的不同层进行不同程度的剪枝，以实现更高效的模型压缩和加速。</li>
<li>网络剪裁（Network Trimming）：通过对模型中一些不重要的连接进行剪裁，从而减小计算开销。</li>
</ol>
</li>
<li><p>量化：</p>
<ol>
<li>模型量化（Model Quantization）：将模型的权重和激活函数的精度从32位浮点数减少到更小的位数，从而减小模型的大小和计算开销。</li>
</ol>
</li>
<li><p>分解：</p>
<ol>
<li>低秩分解（Low-Rank Decomposition）：通过将一个较大的权重矩阵分解为几个较小的权重矩阵，从而减少计算开销。</li>
<li>卷积分解（Convolution Decomposition）：将卷积层分解成几个更小的卷积层或全连接层，以减小计算开销。</li>
</ol>
</li>
<li><p>蒸馏：</p>
<ol>
<li>知识蒸馏（Knowledge Distillation）：利用一个较大、较准确的模型的预测结果来指导一个较小、较简单的模型学习。这种方法可以减小模型的复杂度，提高模型的泛化能力和推理速度。</li>
<li>蒸馏对抗网络（Distillation Adversarial Networks）：在知识蒸馏的基础上，通过对抗训练来提高模型的鲁棒性和抗干扰能力。</li>
</ol>
</li>
</ol>
<h2 id="5-你了解的知识蒸馏模型有哪些？"><a href="#5-你了解的知识蒸馏模型有哪些？" class="headerlink" title="5. 你了解的知识蒸馏模型有哪些？"></a>5. 你了解的知识蒸馏模型有哪些？</h2><ol>
<li>FitNets：使用一个大型模型作为教师模型来指导一个小型模型的训练。</li>
<li>Hinton蒸馏：使用一个大型模型的输出作为标签来指导一个小型模型的训练。</li>
<li>Born-Again Network（BAN）：使用一个已经训练好的模型来初始化一个新模型，然后使用少量的数据重新训练模型。</li>
<li>TinyBERT：使用一个大型BERT模型作为教师模型来指导一个小型BERT模型的训练。</li>
</ol>
<h1 id="19-RAG面"><a href="#19-RAG面" class="headerlink" title="19. RAG面"></a>19. RAG面</h1><h2 id="1-RAG-思路是怎么样？"><a href="#1-RAG-思路是怎么样？" class="headerlink" title="1. RAG 思路是怎么样？"></a>1. RAG 思路是怎么样？</h2><ol>
<li>加载文件</li>
<li>读取文本</li>
<li>文本分割</li>
<li>文本向量化</li>
<li>问句向量化</li>
<li>在文本向量中匹配出与问句向量最相似的 top k 个</li>
<li>匹配出的文本作为上下文和问题一起添加到 prompt 中</li>
<li>提交给 LLM 生成回答</li>
</ol>
<h2 id="2-文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失"><a href="#2-文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失" class="headerlink" title="2. 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失"></a>2. 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失</h2><ol>
<li>将所有的文本组织成二级索引，第一级索引是 [关键信息]，第二级是 [原始文本]，二者一一映射。</li>
<li>检索部分只对关键信息做 embedding，参与相似度计算，把召回结果映射的 原始文本交给LLM。</li>
</ol>
<h2 id="3-在基于垂直领域表现不佳"><a href="#3-在基于垂直领域表现不佳" class="headerlink" title="3. 在基于垂直领域表现不佳"></a>3. 在基于垂直领域表现不佳</h2><ol>
<li>一个是对 embedding 模型的基于垂直领域的数据进行微调；</li>
<li>一个是对 LLM 模型的基于垂直领域的数据进行微调；</li>
</ol>
<h2 id="4-langchain-内置问答分句效果不佳问题"><a href="#4-langchain-内置问答分句效果不佳问题" class="headerlink" title="4. langchain 内置问答分句效果不佳问题"></a>4. langchain 内置问答分句效果不佳问题</h2><ol>
<li>一种是使用更好的文档拆分的方式（如项目中已经集成的达摩院的语义识别的模型及进行拆分）；</li>
<li>一种是改进填充的方式，判断中心句上下文的句子是否和中心句相关，仅添加相关度高的句子；</li>
<li>另一种是文本分段后，对每段分别及进行总结，基于总结内容语义及进行匹配；</li>
</ol>
<h2 id="5-如何尽可能召回与-query-相关的-Document-问题"><a href="#5-如何尽可能召回与-query-相关的-Document-问题" class="headerlink" title="5. 如何尽可能召回与 query 相关的 Document 问题"></a>5. 如何尽可能召回与 query 相关的 Document 问题</h2><ol>
<li>本地知识的内容最好是已经结构化比较好了，各个段落之间语义关联没那么强。</li>
<li>使用 Faiss 做搜索，由Facebook（现Meta）开发的开源库，主要用于高效地进行大规模相似性搜索和聚类操作。</li>
</ol>
<h2 id="6-embedding-模型在表示-text-chunks-时偏差太大问题"><a href="#6-embedding-模型在表示-text-chunks-时偏差太大问题" class="headerlink" title="6. embedding 模型在表示 text chunks 时偏差太大问题"></a>6. embedding 模型在表示 text chunks 时偏差太大问题</h2><p>问题：</p>
<ol>
<li>一些开源的 embedding 模型本身效果一般，尤其是当 text chunk 很大的时候，强行变成一个简单的 vector 是很难准确表示的，开源的模型在效果上确实不如 openai Embeddings；</li>
<li>多语言问题，paper 的内容是英文的，用户的 query 和生成的内容都是中文的，这里有个语言之间的对齐问题，尤其是可以用中文的 query embedding 来从英文的 text chunking embedding 中找到更加相似的 top-k 是个具有挑战的问题</li>
</ol>
<p>方法：</p>
<ol>
<li>用更小的 text chunk 配合更大的 topk 来提升表现，毕竟 smaller text chunk 用 embedding 表示起来 noise 更小，更大的 topk 可以组合更丰富的 context 来生成质量更高的回答；</li>
<li>多语言的问题，可以找一些更加适合多语言的 embedding 模型</li>
</ol>
<h2 id="7-RAG-有哪些评估方法？"><a href="#7-RAG-有哪些评估方法？" class="headerlink" title="7. RAG 有哪些评估方法？"></a>7. RAG 有哪些评估方法？</h2><ol>
<li>独立评估：<ol>
<li>检索模块：用于衡量系统（如搜索引擎、推荐系统或信息检索系统）在根据查询或任务排名项目的有效性。指标：命中率 (Hit Rate)、平均排名倒数 (MRR)、归一化折扣累积增益 (NDCG)，精确度 (Precision) 等。</li>
<li>生成模块：指标：关注上下文相关性，即检索到的文档与查询问题的关联度。</li>
</ol>
</li>
<li>端到端评估：<ol>
<li>对 RAG 模型对特定输入生成的最终响应进行评估，涉及模型生成的答案与输入查询的相关性和一致性。</li>
<li>无标签的内容评估：评价指标：答案的准确性、相关性和无害性</li>
<li>有标签的内容评估：评价指标：准确率 (Accuracy) 和精确匹配 (EM)</li>
</ol>
</li>
</ol>
<h2 id="8-RAG-有哪些关键指标和能力？"><a href="#8-RAG-有哪些关键指标和能力？" class="headerlink" title="8. RAG 有哪些关键指标和能力？"></a>8. RAG 有哪些关键指标和能力？</h2><p>答案的准确性、答案的相关性和上下文的相关性。</p>
<h2 id="9-RAG-有哪些评估框架？"><a href="#9-RAG-有哪些评估框架？" class="headerlink" title="9. RAG 有哪些评估框架？"></a>9. RAG 有哪些评估框架？</h2><ol>
<li>RAGAS：是一个基于简单手写提示的评估框架，通过这些提示全自动地衡量答案的准确性、相关性和上下文相关性。<ol>
<li>答案忠实度评估：利用大语言模型 (LLM) 分解答案为多个陈述，检验每个陈述与上下文的一致性。最终，根据支持的陈述数量与总陈述数量的比例，计算出一个“忠实度得分”。</li>
<li>答案相关性评估：使用大语言模型 (LLM) 创造可能的问题，并分析这些问题与原始问题的相似度。答案相关性得分是通过计算所有生成问题与原始问题相似度的平均值来得出的。</li>
<li>上下文相关性评估：运用大语言模型 (LLM) 筛选出直接与问题相关的句子，以这些句子占上下文总句子数量的比例来确定上下文相关性得分。</li>
</ol>
</li>
<li>ARES：目标是自动化评价 RAG 系统在上下文相关性、答案忠实度和答案相关性三个方面的性能。<ol>
<li>生成合成数据集：ARES 首先使用语言模型从目标语料库中的文档生成合成问题和答案，创建正负两种样本。</li>
<li>训练大语言模型 (LLM) 裁判：然后，ARES 对轻量级语言模型进行微调，利用合成数据集训练它们以评其上下文相关性、答案忠实度和答案相关性。</li>
<li>基于置信区间对 RAG 系统排名：最后，ARES 使用这些裁判模型为 RAG 系统打分，并结合手动标注的验证集，采用生成置信区间，从而可靠地评其 RAG 系统的性能。</li>
</ol>
</li>
</ol>
<h2 id="10-RAG-架构优化有哪些优化策略？"><a href="#10-RAG-架构优化有哪些优化策略？" class="headerlink" title="10. RAG 架构优化有哪些优化策略？"></a>10. RAG 架构优化有哪些优化策略？</h2><ol>
<li><p>利用 知识图谱（KG）进行上下文增强</p>
<ol>
<li>向量数据库进行上下文增强存在问题：无法获取长中关联知识，信息密度低（尤其当 LLM context window 较小时不友好）</li>
<li>利用知识图谱（KG）进行上下文增强的策略，增加一路与向量库平行的 KG（知识图谱）上下文增强策略。</li>
<li>根据 query 抽取实体，然后把实体作为种子节点对图进行采样（必要时，可把 KG中节点和 query 中实体先向量化，通过向量相似度设置种子节点），然后把获取的子图转换成文本片段，从而达到上下文增强的效果。</li>
</ol>
</li>
<li><p>Self-RAG：让大模型对召回结果进行筛选</p>
</li>
<li><p>典型 RAG 架构中，向量数据库存在问题：经典的 RAG 架构中（包括 KG 进行上下文增强），对召回的上下文无差别地与 query 进行合并，然后访问大模型输出应答。但有时召回的上下文可能与 query 无关或者矛盾，此时就应舍弃这个上下文，尤其当大模型上下文窗口较小时非常必要（目前 4k 的窗口比较常见）</p>
</li>
<li><p>Self-RAG 则是更加主动和智能的实现方式，主要步骤概括如下：</p>
<ol>
<li>判断是否需要额外检索事实性信息（retrieve on demand），仅当有需要时才召回；</li>
<li>平行处理每个片段：生产 prompt + 一个片段的生成结果；</li>
<li>使用反思字段，检查输出是否相关，选择最符合需要的片段；</li>
<li>再重复检索；</li>
<li>生成结果会引用相关片段，以及输出结果是否符合该片段，便于查证事实</li>
</ol>
</li>
<li><p>多向量检索器多模态 RAG ：将文档（用于答案合成）和引用（用于检索）分离，这样可以针对不同的数据类型生成适合自然语言检索的摘要，同时保留原始的数据内容。它可以与多模态 LLM 结合，实现跨模态的 RAG。</p>
<ol>
<li>对文本和表格生成 summary，然后应用多模态 embedding 模型把文本&#x2F;表格summary、原始图片转化成 embedding 存入多向量检索器。对话时，根据 query 召回原始文本&#x2F;表格&#x2F;图像。然后将其喂给多模态 LLM 生成应答结果。</li>
<li>首先应用多模态大模型（GPT4-V、LLaVA、FUYU-8b）生成图片 summary。然后对文本&#x2F;表格&#x2F;图片 summary 进行向量化存入多向量检索器中。当生成应答的多模态大模型不具备时，可根据 query 召回原始文本&#x2F;表格+图片 summary。</li>
<li>前置阶段同选项 2 相同。对话时，根据 query 召回原始文本&#x2F;表格&#x2F;图片。构造完整 Prompt，访问多模态大模型生成应答结果。</li>
</ol>
</li>
<li><p>RAG Fusion 优化策略</p>
</li>
<li><p>当接收用户 query 时，让大模型生成 5-10 个相似的 query，然后每个 query 去匹配 5-10 个文本块，接着对所有返回的文本块再做个倒序融合排序，如果有需求就再加个精排，最后取 Top K 个文本块拼接至 prompt。</p>
</li>
<li><p>模块化 RAG 优化策略</p>
<ol>
<li>搜索模块： 融合了直接在（附加的）语料库中进行搜索的方法。这些方法包括利用大语言模型（LLM）生成的代码、SQL、Cypher 等查询语言，或是其他定制工具。其搜索数据源多样，涵盖搜索引擎、文本数据、表格数据或知识图等。</li>
<li>记忆模块： 本模块充分利用大语言模型本身的记忆功能来引导信息检索。其核心原则是寻找与当前输入最为匹配的记忆。这种增强检索的生成模型能够利用其自身的输出来自我提升，在推理过中中使文本更加贴近数据分布，而非仅依赖训练数据。</li>
<li>额外生成模块： 面对检索内容中的冗余和噪声问题，这个模块通过大语言模型生成必要的上下文，而非直接从数据源进行检索。通过这种方式，由大语言模型生成的内容更可能包含与检索任务相关的信息。</li>
<li>任务适应模块： 该模块致力于将 RAG 调整以适应各种下游任务。</li>
<li>对齐模块： 在 RAG 的应用中，查询与文本之间的对齐一直是影响效果的关键因素。在模块化 RAG 的发展中，研究者们发现，在检索器中添加一个可训练的 Adapter 模块能有效解决对齐问题。</li>
<li>验证模块： 在现实世界中，我们无法总是保证检索到的信息的可靠性。检索到不相关的数据可能会导致大语言模型产生错误信息。因此，可以在检索文档后加入一个额外的验证模块，以评其检索到的文档与查询之间的相关性，这样做可以提升 RAG 的鲁棒性。</li>
</ol>
</li>
<li><p>RAG 新模式优化策略</p>
<ol>
<li>RRR 提出了一种重写 - 检索 - 阅读的流中，其中利用大语言模型（LLM）的性能作为强化学习中重写模块的奖励机制。这样，重写模块可以调整检索查询，从而提高阅读器在后续任务中的表现。</li>
</ol>
</li>
<li><p>RAG 结合 SFT</p>
<ol>
<li>更新 LLM。以最大限度地提高在给定检索增强指令的情况下正确答案的概率；</li>
<li>更新检索器。以最大限度地减少文档与查询在语义上相似（相关）的程度。</li>
<li>优点：通过这种方式，使 LLM 更好地利用相关背景知识，并训练 LLM 即使在检索错误块的情况下也能产生准确的预测，从而使模型能够依赖自己的知识。</li>
</ol>
</li>
<li><p>查询转换（Query Transformations）：在某些情况下，用户的 query 可能出现表述不清、需求复杂、内容无关等问题；</p>
<ol>
<li>利用了大型语言模型(LLM)的强大能力，通过某种提示或方法将原始的用户问题转换或重写为更合适的、能够更准确地返回所需结果的查询。</li>
<li>LLM 的能力确保了转换后的查询更有可能从文档或数据中获取相关和准确的答案。</li>
<li>查询转换的核心思想：用户的原始查询可能不总是最适合检索的，所以我们需要某种方式来改进或扩展它。</li>
</ol>
</li>
</ol>
<h2 id="11-如何通过混合检索提升-RAG-效果"><a href="#11-如何通过混合检索提升-RAG-效果" class="headerlink" title="11.  如何通过混合检索提升 RAG 效果?"></a>11.  如何通过混合检索提升 RAG 效果?</h2><p>该策略利用了矢量搜索和关键词搜索等不同检索技术的优势，并将它们智能地结合起来。通过这种混合方法，您仍然可以匹配相关关键字，同时保持对查询意图的控制。</p>
<h2 id="12-如何通过重新排名提升-RAG-效果"><a href="#12-如何通过重新排名提升-RAG-效果" class="headerlink" title="12. 如何通过重新排名提升 RAG 效果?"></a>12. 如何通过重新排名提升 RAG 效果?</h2><p>当查询向量存储时，前 K 个结果不一定按最相关的方式排序。当然，它们都是相关的，但在这些相关块中，最相关的块可能是第 5 或第 7 个，而不是第 1 或第 2 个。这就是重新排名的用武之地。重新排名的简单概念是将最相关的信息重新定位到提示的边缘。</p>
<h2 id="13-如何通过添加元数据-提升-RAG-效果"><a href="#13-如何通过添加元数据-提升-RAG-效果" class="headerlink" title="13. 如何通过添加元数据 提升 RAG 效果?"></a>13. 如何通过添加元数据 提升 RAG 效果?</h2><p>将元数据与索引向量结合使用有助于更好地构建它们，同时提高搜索相关性。以下是一些元数据有用的情景：</p>
<ol>
<li>如果你搜索的项目中，时间是一个维度，你可以根据日期元数据进行排序</li>
<li>如果你搜索科学论文，并且你事先知道你要找的信息总是位于特定部分，比如实验部分，你可以将文章部分添加为每个块的元数据并对其进行过滤仅匹配实验</li>
</ol>
<p>元数据很有用，因为它在向量搜索之上增加了一层结构化搜索。</p>
<h2 id="14-如何通过提示压缩提升-RAG-效果"><a href="#14-如何通过提示压缩提升-RAG-效果" class="headerlink" title="14. 如何通过提示压缩提升 RAG 效果?"></a>14. 如何通过提示压缩提升 RAG 效果?</h2><p>研究表明，在检索上下文中的噪声会对 RAG 性能产生不利影响，更精确地说，对由LLM 生成的答案产生不利影响。一些方案建议在检索后再应用一个后处理步骤，以压缩无关上下文，突出重要段落，并减少总体上下文长度。</p>
<h2 id="15-如何通过-查询重写和扩展-提升-RAG-效果"><a href="#15-如何通过-查询重写和扩展-提升-RAG-效果" class="headerlink" title="15. 如何通过 查询重写和扩展 提升 RAG 效果?"></a>15. 如何通过 查询重写和扩展 提升 RAG 效果?</h2><p>当用户与 RAG 交互时，查询结果不一定能获得最佳的回答，并且不能充分表达与向量存储中的文档匹配的结果。为了解决这个问题，在送到 RAG 之前，我们先发生给 LLM 重写此查询。这可以通过添加中间 LLM 调用轻松实现。</p>
<h2 id="16-RAG-未来发展方向"><a href="#16-RAG-未来发展方向" class="headerlink" title="16. RAG 未来发展方向"></a>16. RAG 未来发展方向</h2><ol>
<li>垂直优化：长上下文的处理问题，鲁棒性研究RAG 以及如何保障企业数据安全等</li>
<li>水平优化：多模态</li>
<li>生态系统完善</li>
</ol>
<h1 id="20-多模态面"><a href="#20-多模态面" class="headerlink" title="20. 多模态面"></a>20. 多模态面</h1><h2 id="1-blip2的架构，优势和之前多模态模型的区别？"><a href="#1-blip2的架构，优势和之前多模态模型的区别？" class="headerlink" title="1. blip2的架构，优势和之前多模态模型的区别？"></a>1. blip2的架构，优势和之前多模态模型的区别？</h2><ol>
<li>bilp2是在冻结的图像模型（负责从图像中提取特征，比如vit）和冻结的语言模型（负责生成语言）中间放入一个Q-Former，我们的目标就是训练这个Q-Former。</li>
<li>Q-Former包含图像Transformer和语言Transformer，图像Transformer包含CA和SA，SA和语言Transformer共享参数，CA只接受图像模型提取的图像特征，图像模型的输入是一个查询值，这个查询值将在SA中和自己交互，在CA中和图像特征交互。最后图像Transformer输出一个综合图像特征的向量，同时语言Transformer输入一个文本，进行encode，得到一个文本的向量。然后根据具体的任务选择不同的方式对这两个向量进行操作。最后，Qformer把得到的向量传给冻结的语言模型。</li>
</ol>
<h2 id="2-多模态中常见的SOTA模型有哪些？"><a href="#2-多模态中常见的SOTA模型有哪些？" class="headerlink" title="2. 多模态中常见的SOTA模型有哪些？"></a>2. 多模态中常见的SOTA模型有哪些？</h2><ol>
<li>Vision Transformer (ViT): 将自注意力机制引入计算机视觉领域，通过将图像划分为图像补丁并应用Transformer模型，实现了在图像分类和目标检测等任务上的出色表现。</li>
<li>CLIP (Contrastive Language-Image Pretraining): 结合了图像和文本的对比学习，通过训练一个模型，使其能够根据图像和文本之间的相互关系进行推理，实现了图像与文本之间的联合理解和表示学习。</li>
<li>UNITER (UNiversal Image-Text Representation): 使用Transformer架构，联合学习图像和文本表示，提供了一个通用的图像和文本特征提取框架，适用于多个视觉和语言任务。</li>
<li>LXMERT (Cross-Modal Transformer): 结合了视觉和语言信息，通过Transformer模型对图像和文本进行交互学习，可以用于视觉问答、图像描述生成等任务。</li>
<li>CoCa (Contrastive Captioners)：这是一种融合了单编码器、双编码器和编码器-解码器三种结构的多模态模型，既能生成图像侧和文本侧独立的表示，又能进行更深层次的图像、文本信息融合以及文本生成。CoCa在图像分类、图文检索、看图说话、VQA等多个任务上都取得了SOTA效果。</li>
</ol>
<h2 id="3-介绍一下stable-diffusion的原理？"><a href="#3-介绍一下stable-diffusion的原理？" class="headerlink" title="3. 介绍一下stable diffusion的原理？"></a>3. 介绍一下stable diffusion的原理？</h2><p>stable diffusion是一种生成模型。其核心思想是通过多次迭代，逐渐将噪声信号演化为目标分布所对应的样本。具体原理如下：</p>
<ol>
<li>初始化噪声信号为服从高斯分布的随机向量。</li>
<li>每一步中，将当前噪声信号与目标分布的梯度信息结合，通过Langevin动力学方程进行更新，使噪声信号逐渐接近目标分布。</li>
<li>迭代的次数越多，噪声信号越接近目标分布，并最终生成目标分布的样本。stable diffusion通过合理的选择演化步长和迭代次数，可以在生成样本的过程中平衡样本质量和生成速度。</li>
</ol>
<h1 id="21-代码面"><a href="#21-代码面" class="headerlink" title="21. 代码面"></a>21. 代码面</h1><h2 id="1-RMS-Norm-相比于-Layer-Norm-有什么特点？"><a href="#1-RMS-Norm-相比于-Layer-Norm-有什么特点？" class="headerlink" title="1.  RMS Norm 相比于 Layer Norm 有什么特点？"></a>1.  RMS Norm 相比于 Layer Norm 有什么特点？</h2><ol>
<li>RMS Norm 简化了 Layer Norm ，去除掉计算均值进行平移的部分。</li>
<li>对比 LN，RMS Norm 的计算速度更快。效果基本相当，甚至略有提升。</li>
</ol>
<h2 id="2-Deep-Norm-思路？"><a href="#2-Deep-Norm-思路？" class="headerlink" title="2.  Deep Norm 思路？"></a>2.  Deep Norm 思路？</h2><ol>
<li>Deep Norm 方法在执行 Layer Norm 之前，up-scale 了残差连接 (alpha&gt;1)；</li>
<li>在初始化阶段 down-scale 了模型参数(beta&lt;1)。</li>
</ol>
<h2 id="3-Deep-Norm-有什么优点？"><a href="#3-Deep-Norm-有什么优点？" class="headerlink" title="3. Deep Norm 有什么优点？"></a>3. Deep Norm 有什么优点？</h2><p>Deep Norm 可以缓解爆炸式模型更新的问题，把模型更新限制在常数，使得模型训练过程更稳定</p>
<h2 id="4-LN-在-LLMs-中的不同位置有什么区别么？如有能介绍一下区别么？"><a href="#4-LN-在-LLMs-中的不同位置有什么区别么？如有能介绍一下区别么？" class="headerlink" title="4. LN 在 LLMs 中的不同位置有什么区别么？如有能介绍一下区别么？"></a>4. LN 在 LLMs 中的不同位置有什么区别么？如有能介绍一下区别么？</h2><ol>
<li>Post LN：<br>位置：layer norm 在残差链接之后<br>缺点：Post LN 在深层的梯度范式逐渐增大，导致使用 post-LN 的深层 transformer 容易出现训练不稳定的问题</li>
<li>Pre-LN：<br>位置：layer norm 在残差链接中<br>优点：相比于 Post-LN，Pre LN 在深层的梯度范式近似相等，所以使用 Pre-LN 的深层transformer 训练更稳定，可以缓解训练不稳定问题<br>缺点：相比于 Post-LN，Pre-LN 的模型效果略差</li>
<li>Sandwich-LN：<br>位置：在 pre-LN 的基础上，额外插入了一个 layer norm<br>优点：Cogview 用来避免值爆炸的问题<br>缺点：训练不稳定，可能会导致训练崩溃。</li>
</ol>
<h2 id="5-LLMs-各模型分别用了哪种-Layer-normalization？"><a href="#5-LLMs-各模型分别用了哪种-Layer-normalization？" class="headerlink" title="5. LLMs 各模型分别用了哪种 Layer normalization？"></a>5. LLMs 各模型分别用了哪种 Layer normalization？</h2><p><img src="/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250503204636473.png" alt="image-20250503204636473"></p>
<h2 id="6-各-LLMs-都使用哪种激活函数？"><a href="#6-各-LLMs-都使用哪种激活函数？" class="headerlink" title="6. 各 LLMs 都使用哪种激活函数？"></a>6. 各 LLMs 都使用哪种激活函数？</h2><p><img src="/images/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/image-20250503204728874.png" alt="image-20250503204728874"></p>
<h2 id="7-传统-Attention-存在哪些问题？"><a href="#7-传统-Attention-存在哪些问题？" class="headerlink" title="7. 传统 Attention 存在哪些问题？"></a>7. 传统 Attention 存在哪些问题？</h2><ol>
<li>传统 Attention 存在上下文长度约束问题；</li>
<li>传统 Attention 速度慢，内存占用大；</li>
</ol>
<h2 id="8-Attention-优化方向？"><a href="#8-Attention-优化方向？" class="headerlink" title="8. Attention 优化方向？"></a>8. Attention 优化方向？</h2><ol>
<li>提升上下文长度</li>
<li>加速、减少内存占用</li>
</ol>
<h2 id="9-Attention-变体有哪些？"><a href="#9-Attention-变体有哪些？" class="headerlink" title="9. Attention 变体有哪些？"></a>9. Attention 变体有哪些？</h2><ol>
<li>稀疏 attention。将稀疏偏差引入 attention 机制可以降低了复杂性；</li>
<li>线性化 attention。解开 attention 矩阵与内核特征图，然后以相反的顺序计算 attention 以实现线性复杂度；</li>
<li>原型和内存压缩。这类方法减少了查询或键值记忆对的数量，以减少注意力矩阵的大小；</li>
<li>低阶 self-Attention。这一系列工作捕获了 self-Attention 的低阶属性；</li>
<li>Attention 与先验。该研究探索了用先验 attention 分布来补充或替代标准 attention；</li>
<li>改进多头机制。该系列研究探索了不同的替代多头机制。</li>
</ol>
<h2 id="10-Multi-head-Attention-存在什么问题？"><a href="#10-Multi-head-Attention-存在什么问题？" class="headerlink" title="10. Multi-head Attention 存在什么问题？"></a>10. Multi-head Attention 存在什么问题？</h2><p>训练过程：不会显著影响训练过程，训练速度不变，会引起非常细微的模型效果损失；<br>推理过程：反复加载巨大的 KV cache , 导致内存开销大，性能是内存受限；</p>
<h2 id="11-介绍一下-Multi-Query-Attention？"><a href="#11-介绍一下-Multi-Query-Attention？" class="headerlink" title="11. 介绍一下 Multi-Query Attention？"></a>11. 介绍一下 Multi-Query Attention？</h2><p>Multi-Query Attention 在所有注意力头上共享 key 和 value</p>
<h2 id="12-对比一下-Multi-head-Attention-和-Multi-Query-Attention？"><a href="#12-对比一下-Multi-head-Attention-和-Multi-Query-Attention？" class="headerlink" title="12. 对比一下 Multi-head Attention 和 Multi-Query Attention？"></a>12. 对比一下 Multi-head Attention 和 Multi-Query Attention？</h2><p>Multi-head Attention：每个注意力头都有各自的 query、key 和 value。<br>Multi-query Attention: 在所有的注意力头上共享 key 和 value。</p>
<h2 id="13-Multi-Query-Attention-这样做的好处是什么？"><a href="#13-Multi-Query-Attention-这样做的好处是什么？" class="headerlink" title="13. Multi-Query Attention 这样做的好处是什么？"></a>13. Multi-Query Attention 这样做的好处是什么？</h2><p>减少 KV cache 的大小，减少显存占用，提升推理速度。</p>
<h2 id="14-有哪些模型是使用-Multi-Query-Attention？"><a href="#14-有哪些模型是使用-Multi-Query-Attention？" class="headerlink" title="14. 有哪些模型是使用 Multi-Query Attention？"></a>14. 有哪些模型是使用 Multi-Query Attention？</h2><p>代表模型：PaLM、ChatGLM2、Falcon 等</p>
<h2 id="15-什么是-Grouped-query-Attention？"><a href="#15-什么是-Grouped-query-Attention？" class="headerlink" title="15. 什么是 Grouped-query Attention？"></a>15. 什么是 Grouped-query Attention？</h2><p>Grouped query attention: 介于 multi head 和 multi query 之间，多个 key 和 value。</p>
<h2 id="16-有哪些大模型使用-Grouped-query-Attention？"><a href="#16-有哪些大模型使用-Grouped-query-Attention？" class="headerlink" title="16. 有哪些大模型使用 Grouped-query Attention？"></a>16. 有哪些大模型使用 Grouped-query Attention？</h2><p>ChatGLM2，LLaMA2-34B&#x2F;70B 使用了 Grouped query attention。</p>
<h2 id="17-Flash-Attention-篇"><a href="#17-Flash-Attention-篇" class="headerlink" title="17. Flash Attention 篇"></a>17. Flash Attention 篇</h2><p>核心：用分块 softmax 等价替代传统 softmax<br>优点：节约 HBM，高效利用 SRAM，省显存，提速度<br>代表模型：Meta 推出的开源大模型 LLaMA，阿联酋推出的开源大模型 Falcon 都使用了 Flash Attention 来加速计算和节省显存<br>关键词：HBM、SRAM、分块 Softmax、重计算、Kernel 融合。</p>
<h2 id="18-并行-transformer-block"><a href="#18-并行-transformer-block" class="headerlink" title="18. 并行 transformer block"></a>18. 并行 transformer block</h2><p>用并行公式替换了串行，提升了 15%的训练速度。在 8B 参数量规模，会有轻微的模型效果损失;在 62B 参数量规模，就不会损失模型效果。Falcon、PaLM 都使用了该技术来加速训练</p>
<h2 id="19-KL-散度与交叉熵的区别？"><a href="#19-KL-散度与交叉熵的区别？" class="headerlink" title="19. KL 散度与交叉熵的区别？"></a>19. KL 散度与交叉熵的区别？</h2><ol>
<li>KL 散度指的是相对熵，KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度越小表示两个分布越接近。也就是说 KL 散度是不对称的，且 KL 散度的值是非负数。（也就是熵和交叉熵的差）</li>
<li>交叉熵损失函数是二分类问题中最常用的损失函数，由于其定义出于信息学的角度，可以泛化到多分类问题中。</li>
</ol>
<p>KL 散度是一种用于衡量两个分布之间差异的指标，交叉熵损失函数是 KL 散度的一种特殊形式。在二分类问题中，交叉熵函数只有一项，而在多分类问题中有多项。</p>
<h2 id="20-多任务学习各-loss-差异过大怎样处理？"><a href="#20-多任务学习各-loss-差异过大怎样处理？" class="headerlink" title="20. 多任务学习各 loss 差异过大怎样处理？"></a>20. 多任务学习各 loss 差异过大怎样处理？</h2><p>多任务学习中，如果各任务的损失差异过大，可以通过动态调整损失权重、使用任务特定的损失函数、改变模型架构或引入正则化等方法来处理。目标是平衡各任务的贡献，以便更好地训练模型。</p>
<h2 id="21-分类问题为什么用交叉熵损失函数不用均方误差（MSE）？"><a href="#21-分类问题为什么用交叉熵损失函数不用均方误差（MSE）？" class="headerlink" title="21. 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？"></a>21. 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？</h2><ol>
<li>交叉熵损失函数通常在分类问题中使用，而均方误差（MSE）损失函数通常用于回归问题。</li>
<li>分类问题的目标是将输入样本分到不同的类别中，输出为类别的概率分布。交叉熵损失函数可以度量两个概率分布之间的差异，使得模型更好地拟合真实的类别分布。它对概率的细微差异更敏感，可以更好地区分不同的类别。此外，交叉熵损失函数在梯度计算时具有较好的数学性质，有助于更稳定地进行模型优化。</li>
<li>相比之下，均方误差（MSE）损失函数更适用于回归问题，其中目标是预测连续数值而不是类别。MSE 损失函数度量预测值与真实值之间的差异的平方，适用于连续数值的回归问题。在分类问题中使用 MSE 损失函数可能不太合适，因为它对概率的微小差异不够敏感，而且在分类问题中通常需要使用激活函数（如 sigmoid 或 softmax）将输出映射到概率空间，使得 MSE 的数学性质不再适用。</li>
</ol>
<h2 id="22-除了-cosin-还有哪些算相似度的方法"><a href="#22-除了-cosin-还有哪些算相似度的方法" class="headerlink" title="22. 除了 cosin 还有哪些算相似度的方法"></a>22. 除了 cosin 还有哪些算相似度的方法</h2><p>除了余弦相似度（cosine similarity）之外，常见的相似度计算方法还包括欧氏距离、曼哈顿距离、Jaccard 相似度、皮尔逊相关系数等。</p>
<h2 id="23-了解对比学习嘛？"><a href="#23-了解对比学习嘛？" class="headerlink" title="23. 了解对比学习嘛？"></a>23. 了解对比学习嘛？</h2><p>对比学习是一种无监督学习方法，通过训练模型使得相同样本的表示更接近，不同样本的表示更远离，从而学习到更好的表示。对比学习通常使用对比损失函数，例如 Siamese 网络、Triplet 网络等，用于学习数据之间的相似性和差异性。</p>
<h2 id="24-对比学习负样本是否重要？负样本构造成本过高应该怎么解决？"><a href="#24-对比学习负样本是否重要？负样本构造成本过高应该怎么解决？" class="headerlink" title="24. 对比学习负样本是否重要？负样本构造成本过高应该怎么解决？"></a>24. 对比学习负样本是否重要？负样本构造成本过高应该怎么解决？</h2><ol>
<li>对比学习中负样本的重要性取决于具体的任务和数据。负样本可以帮助模型学习到样本之间的区分度，从而提高模型的性能和泛化能力。</li>
<li>为了解决负样本构造成本过高的问题，可以考虑以下方法：<ol>
<li>降低负样本的构造成本：通过设计更高效的负样本生成算法或采样策略，减少负样本的构造成本。例如，可以利用数据增强技术生成合成的负样本，或者使用近似采样方法选择与正样本相似但不相同的负样本。</li>
<li>确定关键负样本：根据具体任务的特点，可以重点关注一些关键的负样本，而不是对所有负样本进行详细的构造。这样可以降低构造成本，同时仍然能够有效训练模型。</li>
<li>迁移学习和预训练模型：利用预训练模型或迁移学习的方法，可以在其他领域或任务中利用已有的负样本构造成果，减少重复的负样本构造工作。</li>
</ol>
</li>
</ol>
<h2 id="25-分布式训练框架选择？"><a href="#25-分布式训练框架选择？" class="headerlink" title="25. 分布式训练框架选择？"></a>25. 分布式训练框架选择？</h2><p>多用 DeepSpeed，少用 Pytorch 原生的 torchrun。在节点数量较少的情况下，使用何种训练框架并不是特别重要；然而，一旦涉及到数百个节点，DeepSpeed 显现出其强大之处，其简便的启动和便于性能分析的特点使其成为理想之选。</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">首页</a></li>
        
          <li><a href="/archives/">归档</a></li>
        
          <li><a href="/tags/">标签</a></li>
        
          <li><a href="/categories/">分类</a></li>
        
          <li><a href="/search/">搜索</a></li>
        
          <li><a target="_blank" rel="noopener" href="https://github.com/hughnwj">项目</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E5%9F%BA%E7%A1%80%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">1. 基础面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%9B%AE%E5%89%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E4%BD%93%E7%B3%BB%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">1. 目前主流的开源模型体系有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-prefix-Decoder-%E5%92%8C-causal-Decoder-%E5%92%8C-Encoder-Decoder-%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2. prefix Decoder 和 causal Decoder 和 Encoder Decoder 区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM%E7%9A%84-%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">3.大模型LLM的 训练目标 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B%E6%98%AF%E5%95%A5%E5%8E%9F%E5%9B%A0%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4. 涌现能力是啥原因？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%B8%BA%E4%BD%95%E7%8E%B0%E5%9C%A8%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%A4%A7%E9%83%A8%E5%88%86%E6%98%AFDecoder-only%E7%BB%93%E6%9E%84%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5. 为何现在的大模型大部分是Decoder only结构？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E8%BF%9B%E9%98%B6%E9%9D%A2"><span class="toc-number">2.</span> <span class="toc-text">2. 进阶面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">1. 什么是 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">2. 为什么会出现 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3-LLMs-%E5%A4%8D%E8%AF%BB%E6%9C%BA%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">2.3.</span> <span class="toc-text">3. 如何缓解 LLMs 复读机问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E7%94%A8Bert%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%80%E4%B9%88%E6%83%85%E5%86%B5%E7%94%A8LLaMA%E3%80%81ChatGLM%E7%B1%BB%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%92%8B%E9%80%89%EF%BC%9F"><span class="toc-number">2.4.</span> <span class="toc-text">4. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E8%AE%A9%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%A4%84%E7%90%86%E6%9B%B4%E9%95%BF%E7%9A%84%E6%96%87%E6%9C%AC"><span class="toc-number">2.5.</span> <span class="toc-text">5. 如何让大模型处理更长的文本</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E8%AF%84%E6%B5%8B%E9%9D%A2"><span class="toc-number">3.</span> <span class="toc-text">3. 评测面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%80%8E%E4%B9%88%E8%AF%84%E6%B5%8B"><span class="toc-number">3.1.</span> <span class="toc-text">1. 大模型怎么评测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2. 大模型评估方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%B7%A5%E5%85%B7%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">3. 大模型评估工具有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-agent%E9%9D%A2"><span class="toc-number">4.</span> <span class="toc-text">4. agent面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-agent-%E6%9C%89%E5%93%AA%E4%BA%9B%E9%83%A8%E5%88%86%E7%BB%84%E6%88%90"><span class="toc-number">4.1.</span> <span class="toc-text">1. agent 有哪些部分组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BB%80%E4%B9%88%E6%98%AFPlanning"><span class="toc-number">4.2.</span> <span class="toc-text">2. 什么是Planning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E5%B9%BB%E8%A7%89%E9%9D%A2"><span class="toc-number">5.</span> <span class="toc-text">5. 幻觉面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1. 什么是大模型幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88LLM%E4%BC%9A%E4%BA%A7%E7%94%9F%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">2. 为什么LLM会产生幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3LLM%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3. 如何缓解LLM幻觉？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-LLMs%E4%BB%80%E4%B9%88%E6%97%B6%E5%80%99%E6%9C%80%E5%AE%B9%E6%98%93%E4%BA%A7%E7%94%9F%E5%B9%BB%E8%A7%89%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4. LLMs什么时候最容易产生幻觉？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-RLHF%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.</span> <span class="toc-text">6. 强化学习-RLHF人类反馈强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-LLM%E7%9A%84%E7%BB%8F%E5%85%B8%E9%A2%84%E8%AE%AD%E7%BB%83Pipeline%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">1. 介绍一下 LLM的经典预训练Pipeline？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%85%B7%E4%BD%93%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2. 具体介绍一下预训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%85%B7%E4%BD%93%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%9F"><span class="toc-number">6.3.</span> <span class="toc-text">3. 具体介绍一下有监督微调？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E5%AF%B9%E9%BD%90%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4. 简单介绍一下对齐？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-RLHF-%E6%B5%81%E7%A8%8B%EF%BC%9F"><span class="toc-number">6.5.</span> <span class="toc-text">5. 简单介绍一下 RLHF 流程？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A6%82%E4%BD%95%E5%9C%A8%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E4%B8%8A%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AARM%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">6.6.</span> <span class="toc-text">6. 如何在有监督微调模型基础上创建一个RM模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BRLHF%E4%B8%AD%E7%9A%84PPO%E4%B8%BB%E8%A6%81%E5%88%86%E5%93%AA%E4%BA%9B%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="toc-number">6.7.</span> <span class="toc-text">7. 大语言模型RLHF中的PPO主要分哪些步骤？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%B8%BE%E4%BE%8B%E6%8F%8F%E8%BF%B0%E4%B8%80%E4%B8%8B%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84RLHF%EF%BC%9F"><span class="toc-number">6.8.</span> <span class="toc-text">8. 举例描述一下大语言模型的RLHF？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-PPO%E9%87%87%E6%A0%B7%E8%BF%87%E7%A8%8B"><span class="toc-number">6.9.</span> <span class="toc-text">8. PPO采样过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-PPO-%E4%B8%AD-%E9%87%87%E6%A0%B7%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-number">6.10.</span> <span class="toc-text">9.介绍一下 PPO 中 采样策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-instructGPT%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%8C%E8%AE%B2%E8%AE%B2rlhf%E5%92%8Creward%EF%BC%9F"><span class="toc-number">6.11.</span> <span class="toc-text">10. instructGPT的原理，讲讲rlhf和reward？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E8%AE%AD%E7%BB%83%E9%9B%86%E9%9D%A2"><span class="toc-number">7.</span> <span class="toc-text">7. 训练集面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT%EF%BC%88%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%89%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F%EF%BC%9F"><span class="toc-number">7.1.</span> <span class="toc-text">1. SFT（有监督微调）的数据集格式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-RM%EF%BC%88%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%EF%BC%89%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">2. RM（奖励模型）的数据格式？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E6%8E%A8%E7%90%86%E9%9D%A2"><span class="toc-number">8.</span> <span class="toc-text">8. 推理面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E4%BD%95%E4%BC%B0%E7%AE%97%E6%A8%A1%E5%9E%8B%E6%89%80%E9%9C%80%E7%9A%84RAM%EF%BC%9F"><span class="toc-number">8.1.</span> <span class="toc-text">1. 如何估算模型所需的RAM？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-%E5%BE%AE%E8%B0%83%E9%9D%A2"><span class="toc-number">9.</span> <span class="toc-text">9. 微调面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%A2%86%E5%9F%9F%E6%95%B0%E6%8D%AE%E8%AE%AD%E7%BB%83%E5%90%8E%EF%BC%8C%E9%80%9A%E7%94%A8%E8%83%BD%E5%8A%9B%E5%BE%80%E5%BE%80%E4%BC%9A%E6%9C%89%E6%89%80%E4%B8%8B%E9%99%8D%EF%BC%8C%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3%E6%A8%A1%E5%9E%8B%E9%81%97%E5%BF%98%E9%80%9A%E7%94%A8%E8%83%BD%E5%8A%9B%EF%BC%9F"><span class="toc-number">9.1.</span> <span class="toc-text">1. 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%87%BA%E7%8E%B0%E8%83%BD%E5%8A%9B%E5%8A%A3%E5%8C%96%EF%BC%8C%E7%81%BE%E9%9A%BE%E6%80%A7%E9%81%97%E5%BF%98%E6%98%AF%E6%80%8E%E4%B9%88%E5%9B%9E%E4%BA%8B%EF%BC%9F"><span class="toc-number">9.2.</span> <span class="toc-text">2.  微调后的模型出现能力劣化，灾难性遗忘是怎么回事？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C-batch-size-%E8%AE%BE%E7%BD%AE%E5%A4%AA%E5%B0%8F-%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">9.3.</span> <span class="toc-text">3. 微调大模型时，如果 batch size 设置太小 会出现什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C-batch-size-%E8%AE%BE%E7%BD%AE%E5%A4%AA%E5%A4%A7-%E4%BC%9A%E5%87%BA%E7%8E%B0%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">9.4.</span> <span class="toc-text">4.  微调大模型时，如果 batch size 设置太大 会出现什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%BE%AE%E8%B0%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%97%B6-%E4%BC%98%E5%8C%96%E5%99%A8%E5%A6%82%E4%BD%95%EF%BC%9F"><span class="toc-number">9.5.</span> <span class="toc-text">5.  微调大模型时, 优化器如何？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83loss%E7%AA%81%E5%88%BA%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">9.6.</span> <span class="toc-text">6. 大模型训练loss突刺是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%BC%9A%E5%87%BA%E7%8E%B0loss%E7%AA%81%E5%88%BA%EF%BC%9F"><span class="toc-number">9.7.</span> <span class="toc-text">7. 为什么大模型训练会出现loss突刺？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83loss%E7%AA%81%E5%88%BA-%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">9.8.</span> <span class="toc-text">8. 大模型训练loss突刺 如何解决？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-LoRA%E9%9D%A2"><span class="toc-number">10.</span> <span class="toc-text">10. LoRA面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-LoRA%EF%BC%9F"><span class="toc-number">10.1.</span> <span class="toc-text">1.  什么是 LoRA？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.2.</span> <span class="toc-text">2. LoRA 的思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-LoRA-%E7%9A%84%E7%89%B9%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.3.</span> <span class="toc-text">3. LoRA 的特点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-QLoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">10.4.</span> <span class="toc-text">4. QLoRA 的思路是怎么样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-AdaLoRA-%E7%9A%84%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%E7%9A%84%EF%BC%9F"><span class="toc-number">10.5.</span> <span class="toc-text">5. AdaLoRA 的思路是怎么样的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-LoRA%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95%E4%B8%BA%E5%95%A5%E8%83%BD%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">10.6.</span> <span class="toc-text">6. LoRA微调方法为啥能加速训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%B2%E6%9C%89LoRA%E6%A8%A1%E5%9E%8B%E4%B8%8A%E7%BB%A7%E7%BB%AD%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">10.7.</span> <span class="toc-text">7.  如何在已有LoRA模型上继续训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-LoRA-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.8.</span> <span class="toc-text">8. LoRA 缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Rank-%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%EF%BC%9F"><span class="toc-number">10.9.</span> <span class="toc-text">9. Rank 如何选取？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-alpha%E5%8F%82%E6%95%B0-%E5%A6%82%E4%BD%95%E9%80%89%E5%8F%96%EF%BC%9F"><span class="toc-number">10.10.</span> <span class="toc-text">10. alpha参数 如何选取？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-LoRA-%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83-%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9F"><span class="toc-number">10.11.</span> <span class="toc-text">11. LoRA 高效微调 如何避免过拟合？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Lora%E7%9A%84%E7%9F%A9%E9%98%B5%E6%80%8E%E4%B9%88%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">10.12.</span> <span class="toc-text">12. Lora的矩阵怎么初始化？为什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">11. 提示学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89%EF%BC%9F"><span class="toc-number">11.1.</span> <span class="toc-text">1. 什么是 提示学习（Prompting）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89%EF%BC%9F"><span class="toc-number">11.2.</span> <span class="toc-text">2. 为什么需要 提示学习（Prompting）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%EF%BC%88Prompting%EF%BC%89-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9%EF%BC%9F"><span class="toc-number">11.3.</span> <span class="toc-text">3. 提示学习（Prompting） 有什么优点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%EF%BC%9F"><span class="toc-number">11.4.</span> <span class="toc-text">4. 为什么需要 前缀微调（Prefix-tining）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.5.</span> <span class="toc-text">5. 前缀微调（Prefix-tining）思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E7%9A%84%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.6.</span> <span class="toc-text">6. 前缀微调（Prefix-tining）的优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%88Prefix-tining%EF%BC%89%E7%9A%84%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.7.</span> <span class="toc-text">7.  前缀微调（Prefix-tining）的缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%EF%BC%9F"><span class="toc-number">11.8.</span> <span class="toc-text">8. 为什么需要 指示微调（Prompt-tuning）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.9.</span> <span class="toc-text">9. 指示微调（Prompt-tuning）思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.10.</span> <span class="toc-text">10. 指示微调（Prompt-tuning）优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.11.</span> <span class="toc-text">11. 指示微调（Prompt-tuning）缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%B8%8E-Prefix-tuning-%E5%8C%BA%E5%88%AB-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.12.</span> <span class="toc-text">12.  指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E6%8C%87%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%88Prompt-tuning%EF%BC%89%E4%B8%8E-fine-tuning-%E5%8C%BA%E5%88%AB-%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.13.</span> <span class="toc-text">13. 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-P-tuning%EF%BC%9F"><span class="toc-number">11.14.</span> <span class="toc-text">14. 为什么需要 P-tuning？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-P-tuning-%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.15.</span> <span class="toc-text">15. P-tuning 思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-P-tuning-%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.16.</span> <span class="toc-text">16. P-tuning 优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-P-tuning-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.17.</span> <span class="toc-text">17. P-tuning 缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-P-tuning-v2%EF%BC%9F"><span class="toc-number">11.18.</span> <span class="toc-text">18. 为什么需要 P-tuning v2？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-P-tuning-v2-%E6%80%9D%E8%B7%AF%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.19.</span> <span class="toc-text">19. P-tuning v2 思路是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-P-tuning-v2-%E4%BC%98%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.20.</span> <span class="toc-text">20. P-tuning v2 优点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-P-tuning-v2-%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">11.21.</span> <span class="toc-text">21. P-tuning v2 缺点是什么？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#12-langchain%E9%9D%A2"><span class="toc-number">12.</span> <span class="toc-text">12. langchain面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Components-and-Chains"><span class="toc-number">12.1.</span> <span class="toc-text">1. Components and Chains</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Prompt-Templates-and-Values"><span class="toc-number">12.2.</span> <span class="toc-text">2. Prompt Templates and Values</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Example-Selectors"><span class="toc-number">12.3.</span> <span class="toc-text">3. Example Selectors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Output-Parsers"><span class="toc-number">12.4.</span> <span class="toc-text">4. Output Parsers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Indexes-and-Retrievers"><span class="toc-number">12.5.</span> <span class="toc-text">5. Indexes and Retrievers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Chat-Message-History"><span class="toc-number">12.6.</span> <span class="toc-text">6. Chat Message History</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Agents-and-Toolkits"><span class="toc-number">12.7.</span> <span class="toc-text">7. Agents and Toolkits</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-LangChain-%E5%A6%82%E4%BD%95Embedding-vector-store%EF%BC%9F"><span class="toc-number">12.8.</span> <span class="toc-text">8. LangChain 如何Embedding &amp; vector store？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-Tokenizer%E9%9D%A2"><span class="toc-number">13.</span> <span class="toc-text">13. Tokenizer面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Tokenizer-%E4%BB%8B%E7%BB%8D"><span class="toc-number">13.1.</span> <span class="toc-text">1. Tokenizer 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Byte-Pair-Encoding-BPE"><span class="toc-number">13.2.</span> <span class="toc-text">2. Byte-Pair Encoding(BPE)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-WordPiece-%E4%B8%8E-BPE-%E5%BC%82%E5%90%8C%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">13.3.</span> <span class="toc-text">3. WordPiece 与 BPE 异同点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-SentencePiece-%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="toc-number">13.4.</span> <span class="toc-text">4. 简单介绍一下 SentencePiece 思路？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E9%9D%A2"><span class="toc-number">14.</span> <span class="toc-text">14. 分布式训练面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%A6%82%E6%9E%9C%E6%9C%89N%E5%BC%A0%E6%98%BE%E5%AD%98%E8%B6%B3%E5%A4%9F%E5%A4%A7%E7%9A%84%E6%98%BE%E5%8D%A1%EF%BC%8C%E6%80%8E%E4%B9%88%E5%8A%A0%E9%80%9F%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">14.1.</span> <span class="toc-text">1. 如果有N张显存足够大的显卡，怎么加速训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A6%82%E6%9E%9C%E6%98%BE%E5%8D%A1%E7%9A%84%E6%98%BE%E5%AD%98%E4%B8%8D%E5%A4%9F%E8%A3%85%E4%B8%8B%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%91%A2%EF%BC%9F"><span class="toc-number">14.2.</span> <span class="toc-text">2. 如果显卡的显存不够装下一个完整的模型呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PP%E6%8E%A8%E7%90%86%E6%97%B6%EF%BC%8C%E6%98%AF%E4%B8%80%E4%B8%AA%E4%B8%B2%E8%A1%8C%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%8C1%E4%B8%AAGPU%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%85%B6%E4%BB%96%E7%A9%BA%E9%97%B2%EF%BC%8C%E6%9C%89%E6%B2%A1%E6%9C%89%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F%EF%BC%9F"><span class="toc-number">14.3.</span> <span class="toc-text">3. PP推理时，是一个串行的过程，1个GPU计算，其他空闲，有没有其他方式？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3D-%E5%B9%B6%E8%A1%8C"><span class="toc-number">14.4.</span> <span class="toc-text">4. 3D 并行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E9%9D%A2"><span class="toc-number">15.</span> <span class="toc-text">15. 推理加速面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BD%93%E5%89%8D%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B%E6%9C%80%E4%B8%BB%E8%A6%81%E6%8A%80%E6%9C%AF%E6%89%8B%E6%AE%B5%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.1.</span> <span class="toc-text">1. 当前优化模型最主要技术手段有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F%E6%A1%86%E6%9E%B6%E6%9C%89%E5%93%AA%E4%B8%80%E4%BA%9B%EF%BC%9F%E9%83%BD%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">15.2.</span> <span class="toc-text">2. 推理加速框架有哪一些？都有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-vLLM-%E7%9A%84-%E5%8A%9F%E8%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.3.</span> <span class="toc-text">3. vLLM 的 功能有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-vLLM-%E7%9A%84-%E4%BC%98%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.4.</span> <span class="toc-text">4. vLLM 的 优点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-vLLM-%E7%9A%84-%E7%BC%BA%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.5.</span> <span class="toc-text">5. vLLM 的 缺点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-Text-generation-inference%EF%BC%9F"><span class="toc-number">15.6.</span> <span class="toc-text">6. 介绍一下 Text generation inference？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Text-generation-inference-%E7%9A%84-%E5%8A%9F%E8%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.7.</span> <span class="toc-text">7. Text generation inference 的 功能有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Text-generation-inference-%E7%9A%84-%E4%BC%98%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.8.</span> <span class="toc-text">8. Text generation inference 的 优点有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Text-generation-inference-%E7%9A%84-%E7%BC%BA%E7%82%B9%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">15.9.</span> <span class="toc-text">9. Text generation inference 的 缺点有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#16-%E6%98%BE%E5%AD%98%E9%97%AE%E9%A2%98%E9%9D%A2"><span class="toc-number">16.</span> <span class="toc-text">16. 显存问题面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-nB-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%98%BE%E5%AD%98%EF%BC%9F"><span class="toc-number">16.1.</span> <span class="toc-text">1. nB 模型训练需要多少显存？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%E9%9D%A2"><span class="toc-number">17.</span> <span class="toc-text">17. 增量预训练面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A2%9E%E9%87%8F%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">17.1.</span> <span class="toc-text">1. 为什么要增量预训练？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#18-%E8%92%B8%E9%A6%8F%E9%9D%A2"><span class="toc-number">18.</span> <span class="toc-text">18. 蒸馏面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E6%A0%B7%E6%9C%AC%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-number">18.1.</span> <span class="toc-text">1. 知识蒸馏和无监督样本训练？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AF%B9%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9F%A5%E9%81%93%E5%A4%9A%E5%B0%91%EF%BC%8C%E6%9C%89%E5%93%AA%E4%BA%9B%E6%94%B9%E8%BF%9B%E7%94%A8%E5%88%B0%E4%BA%86%EF%BC%9F"><span class="toc-number">18.2.</span> <span class="toc-text">2. 对知识蒸馏知道多少，有哪些改进用到了？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%B0%88%E4%B8%80%E4%B8%8B%E5%AF%B9%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E7%9A%84%E4%BA%86%E8%A7%A3%EF%BC%9F"><span class="toc-number">18.3.</span> <span class="toc-text">3. 谈一下对模型量化的了解？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E5%92%8C%E5%8A%A0%E9%80%9F%E7%9A%84%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">18.4.</span> <span class="toc-text">4. 模型压缩和加速的方法有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BD%A0%E4%BA%86%E8%A7%A3%E7%9A%84%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">18.5.</span> <span class="toc-text">5. 你了解的知识蒸馏模型有哪些？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#19-RAG%E9%9D%A2"><span class="toc-number">19.</span> <span class="toc-text">19. RAG面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RAG-%E6%80%9D%E8%B7%AF%E6%98%AF%E6%80%8E%E4%B9%88%E6%A0%B7%EF%BC%9F"><span class="toc-number">19.1.</span> <span class="toc-text">1. RAG 思路是怎么样？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%87%E6%A1%A3%E5%88%87%E5%88%86%E7%B2%92%E5%BA%A6%E4%B8%8D%E5%A5%BD%E6%8A%8A%E6%8E%A7%EF%BC%8C%E6%97%A2%E6%8B%85%E5%BF%83%E5%99%AA%E5%A3%B0%E5%A4%AA%E5%A4%9A%E5%8F%88%E6%8B%85%E5%BF%83%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF%E4%B8%A2%E5%A4%B1"><span class="toc-number">19.2.</span> <span class="toc-text">2. 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%9C%A8%E5%9F%BA%E4%BA%8E%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E8%A1%A8%E7%8E%B0%E4%B8%8D%E4%BD%B3"><span class="toc-number">19.3.</span> <span class="toc-text">3. 在基于垂直领域表现不佳</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-langchain-%E5%86%85%E7%BD%AE%E9%97%AE%E7%AD%94%E5%88%86%E5%8F%A5%E6%95%88%E6%9E%9C%E4%B8%8D%E4%BD%B3%E9%97%AE%E9%A2%98"><span class="toc-number">19.4.</span> <span class="toc-text">4. langchain 内置问答分句效果不佳问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A6%82%E4%BD%95%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%8F%AC%E5%9B%9E%E4%B8%8E-query-%E7%9B%B8%E5%85%B3%E7%9A%84-Document-%E9%97%AE%E9%A2%98"><span class="toc-number">19.5.</span> <span class="toc-text">5. 如何尽可能召回与 query 相关的 Document 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-embedding-%E6%A8%A1%E5%9E%8B%E5%9C%A8%E8%A1%A8%E7%A4%BA-text-chunks-%E6%97%B6%E5%81%8F%E5%B7%AE%E5%A4%AA%E5%A4%A7%E9%97%AE%E9%A2%98"><span class="toc-number">19.6.</span> <span class="toc-text">6. embedding 模型在表示 text chunks 时偏差太大问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-number">19.7.</span> <span class="toc-text">7. RAG 有哪些评估方法？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%85%B3%E9%94%AE%E6%8C%87%E6%A0%87%E5%92%8C%E8%83%BD%E5%8A%9B%EF%BC%9F"><span class="toc-number">19.8.</span> <span class="toc-text">8. RAG 有哪些关键指标和能力？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-RAG-%E6%9C%89%E5%93%AA%E4%BA%9B%E8%AF%84%E4%BC%B0%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="toc-number">19.9.</span> <span class="toc-text">9. RAG 有哪些评估框架？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-RAG-%E6%9E%B6%E6%9E%84%E4%BC%98%E5%8C%96%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-number">19.10.</span> <span class="toc-text">10. RAG 架构优化有哪些优化策略？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.11.</span> <span class="toc-text">11.  如何通过混合检索提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E9%87%8D%E6%96%B0%E6%8E%92%E5%90%8D%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.12.</span> <span class="toc-text">12. 如何通过重新排名提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%B7%BB%E5%8A%A0%E5%85%83%E6%95%B0%E6%8D%AE-%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.13.</span> <span class="toc-text">13. 如何通过添加元数据 提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%8F%90%E7%A4%BA%E5%8E%8B%E7%BC%A9%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.14.</span> <span class="toc-text">14. 如何通过提示压缩提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87-%E6%9F%A5%E8%AF%A2%E9%87%8D%E5%86%99%E5%92%8C%E6%89%A9%E5%B1%95-%E6%8F%90%E5%8D%87-RAG-%E6%95%88%E6%9E%9C"><span class="toc-number">19.15.</span> <span class="toc-text">15. 如何通过 查询重写和扩展 提升 RAG 效果?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-RAG-%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91"><span class="toc-number">19.16.</span> <span class="toc-text">16. RAG 未来发展方向</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#20-%E5%A4%9A%E6%A8%A1%E6%80%81%E9%9D%A2"><span class="toc-number">20.</span> <span class="toc-text">20. 多模态面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-blip2%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%BC%98%E5%8A%BF%E5%92%8C%E4%B9%8B%E5%89%8D%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">20.1.</span> <span class="toc-text">1. blip2的架构，优势和之前多模态模型的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E6%A8%A1%E6%80%81%E4%B8%AD%E5%B8%B8%E8%A7%81%E7%9A%84SOTA%E6%A8%A1%E5%9E%8B%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">20.2.</span> <span class="toc-text">2. 多模态中常见的SOTA模型有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bstable-diffusion%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">20.3.</span> <span class="toc-text">3. 介绍一下stable diffusion的原理？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#21-%E4%BB%A3%E7%A0%81%E9%9D%A2"><span class="toc-number">21.</span> <span class="toc-text">21. 代码面</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-RMS-Norm-%E7%9B%B8%E6%AF%94%E4%BA%8E-Layer-Norm-%E6%9C%89%E4%BB%80%E4%B9%88%E7%89%B9%E7%82%B9%EF%BC%9F"><span class="toc-number">21.1.</span> <span class="toc-text">1.  RMS Norm 相比于 Layer Norm 有什么特点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Deep-Norm-%E6%80%9D%E8%B7%AF%EF%BC%9F"><span class="toc-number">21.2.</span> <span class="toc-text">2.  Deep Norm 思路？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Deep-Norm-%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9%EF%BC%9F"><span class="toc-number">21.3.</span> <span class="toc-text">3. Deep Norm 有什么优点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-LN-%E5%9C%A8-LLMs-%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C%E4%BD%8D%E7%BD%AE%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%E4%B9%88%EF%BC%9F%E5%A6%82%E6%9C%89%E8%83%BD%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B%E5%8C%BA%E5%88%AB%E4%B9%88%EF%BC%9F"><span class="toc-number">21.4.</span> <span class="toc-text">4. LN 在 LLMs 中的不同位置有什么区别么？如有能介绍一下区别么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-LLMs-%E5%90%84%E6%A8%A1%E5%9E%8B%E5%88%86%E5%88%AB%E7%94%A8%E4%BA%86%E5%93%AA%E7%A7%8D-Layer-normalization%EF%BC%9F"><span class="toc-number">21.5.</span> <span class="toc-text">5. LLMs 各模型分别用了哪种 Layer normalization？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%90%84-LLMs-%E9%83%BD%E4%BD%BF%E7%94%A8%E5%93%AA%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">21.6.</span> <span class="toc-text">6. 各 LLMs 都使用哪种激活函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%BC%A0%E7%BB%9F-Attention-%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">21.7.</span> <span class="toc-text">7. 传统 Attention 存在哪些问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Attention-%E4%BC%98%E5%8C%96%E6%96%B9%E5%90%91%EF%BC%9F"><span class="toc-number">21.8.</span> <span class="toc-text">8. Attention 优化方向？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Attention-%E5%8F%98%E4%BD%93%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">21.9.</span> <span class="toc-text">9. Attention 变体有哪些？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Multi-head-Attention-%E5%AD%98%E5%9C%A8%E4%BB%80%E4%B9%88%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">21.10.</span> <span class="toc-text">10. Multi-head Attention 存在什么问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8B-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.11.</span> <span class="toc-text">11. 介绍一下 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-%E5%AF%B9%E6%AF%94%E4%B8%80%E4%B8%8B-Multi-head-Attention-%E5%92%8C-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.12.</span> <span class="toc-text">12. 对比一下 Multi-head Attention 和 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Multi-Query-Attention-%E8%BF%99%E6%A0%B7%E5%81%9A%E7%9A%84%E5%A5%BD%E5%A4%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">21.13.</span> <span class="toc-text">13. Multi-Query Attention 这样做的好处是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A8%A1%E5%9E%8B%E6%98%AF%E4%BD%BF%E7%94%A8-Multi-Query-Attention%EF%BC%9F"><span class="toc-number">21.14.</span> <span class="toc-text">14. 有哪些模型是使用 Multi-Query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-%E4%BB%80%E4%B9%88%E6%98%AF-Grouped-query-Attention%EF%BC%9F"><span class="toc-number">21.15.</span> <span class="toc-text">15. 什么是 Grouped-query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8-Grouped-query-Attention%EF%BC%9F"><span class="toc-number">21.16.</span> <span class="toc-text">16. 有哪些大模型使用 Grouped-query Attention？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-Flash-Attention-%E7%AF%87"><span class="toc-number">21.17.</span> <span class="toc-text">17. Flash Attention 篇</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-%E5%B9%B6%E8%A1%8C-transformer-block"><span class="toc-number">21.18.</span> <span class="toc-text">18. 并行 transformer block</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-KL-%E6%95%A3%E5%BA%A6%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">21.19.</span> <span class="toc-text">19. KL 散度与交叉熵的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%E5%90%84-loss-%E5%B7%AE%E5%BC%82%E8%BF%87%E5%A4%A7%E6%80%8E%E6%A0%B7%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">21.20.</span> <span class="toc-text">20. 多任务学习各 loss 差异过大怎样处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#21-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8D%E7%94%A8%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89%EF%BC%9F"><span class="toc-number">21.21.</span> <span class="toc-text">21. 分类问题为什么用交叉熵损失函数不用均方误差（MSE）？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-%E9%99%A4%E4%BA%86-cosin-%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E7%AE%97%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">21.22.</span> <span class="toc-text">22. 除了 cosin 还有哪些算相似度的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-%E4%BA%86%E8%A7%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%98%9B%EF%BC%9F"><span class="toc-number">21.23.</span> <span class="toc-text">23. 了解对比学习嘛？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%98%AF%E5%90%A6%E9%87%8D%E8%A6%81%EF%BC%9F%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%9E%84%E9%80%A0%E6%88%90%E6%9C%AC%E8%BF%87%E9%AB%98%E5%BA%94%E8%AF%A5%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">21.24.</span> <span class="toc-text">24. 对比学习负样本是否重要？负样本构造成本过高应该怎么解决？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">21.25.</span> <span class="toc-text">25. 分布式训练框架选择？</span></a></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&text=0.大模型面经"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&is_video=false&description=0.大模型面经"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=0.大模型面经&body=Check out this article: http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&title=0.大模型面经"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&name=0.大模型面经&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2025/04/27/0.%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%9D%A2%E7%BB%8F/&t=0.大模型面经"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024-2025
    cozy
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">首页</a></li><!--
     --><!--
       --><li><a href="/archives/">归档</a></li><!--
     --><!--
       --><li><a href="/tags/">标签</a></li><!--
     --><!--
       --><li><a href="/categories/">分类</a></li><!--
     --><!--
       --><li><a href="/search/">搜索</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="https://github.com/hughnwj">项目</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板！\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功！");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
